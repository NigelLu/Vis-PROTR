{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOT = 1                                                                                          # number of shots\n",
    "AUG_TYPE = 0                                                                                      # current best setting is use get_aug_data0()\n",
    "DATA_ROOT = '/Users/nigel/Documents/Research-Git/Dataset/VOCdevkit/VOC2012'                       # the absolute/relative path to your dataset directory\n",
    "COLOR_MODE = 'R'                                                                                  # 'R','G','B' red, green, blue, please use capital letters\n",
    "CONFIG_PATH = '/Users/nigel/Documents/Research-Git/Vis/Config/pascal_aug.yaml'                    # the absolute/relative path to your base config .yaml file\n",
    "SEARCH_MODE = 1                                                                                   # 0: base VS IDA (no LCCA); 1: base VS LCCA (no IDA); 2: base VS IDA+LCCA\n",
    "META_AUG = 2                                                                                      # <= 1 means no additional augmentations will be applied during data loading\n",
    "RESUME_PATH = '/Users/nigel/Documents/Research-Git/Vis/pretrained/pascal/split0/pspnet_resnet50/'  # directory to resume weight path for PSPNet, weight file name should be 'best.pth'\n",
    "CLASS_LIST_PATH = '/Users/nigel/Documents/Research-Git/Vis/lists/json'                            # directory path to JSON list files (please name class files as 'data-list.json' & 'sub-class.json')\n",
    "ATT_TYPE = 3                                                                                      # images used for attention (LCCA)--0: only use original image; 1: only use augmented image; 2: no tensor_slice; 3: adaptive\n",
    "META_MODEL_PATH = '/Users/nigel/Documents/Research-Git/Vis/pretrained/mmn-all/'                   # directory to resume weight path for MMN, note that exact folder name 'f11e_pm10' is not requried as it will be inferred from config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding:utf-8\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import numbers\n",
    "import collections\n",
    "import cv2\n",
    "import torch\n",
    "import PIL\n",
    "import PIL.ImageOps\n",
    "import PIL.ImageEnhance\n",
    "import PIL.ImageDraw\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "# ==================================================================================================\n",
    "# Transforms have been borrowed from https://github.com/hszhao/semseg/blob/master/util/py\n",
    "# ==================================================================================================\n",
    "PARAMETER_MAX = 10\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, segtransform):\n",
    "        self.segtransform = segtransform\n",
    "\n",
    "    def __call__(self, image, label=None):\n",
    "        if label is None:\n",
    "            for t in self.segtransform:\n",
    "                image = t(image, None)\n",
    "            return image\n",
    "        else:\n",
    "            for t in self.segtransform:\n",
    "                image, label = t(image, label)\n",
    "            return image, label\n",
    "\n",
    "\n",
    "class ToTensorPIL(object):\n",
    "    # Converts numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).\n",
    "    def __call__(self, image, label):\n",
    "        image = transforms.ToTensor()(image)\n",
    "\n",
    "        if label is not None:\n",
    "            if not isinstance(label, np.ndarray):\n",
    "                raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray\"\n",
    "                                    \"[eg: data readed by cv2.imread()].\\n\"))\n",
    "            if not len(label.shape) == 2:\n",
    "                raise (RuntimeError(\n",
    "                    \"segtransform.ToTensor() only handle np.ndarray labellabel with 2 dims.\\n\"))\n",
    "            label = torch.from_numpy(label)\n",
    "            if not isinstance(label, torch.LongTensor):\n",
    "                label = label.long()\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    # Converts numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).\n",
    "    def __call__(self, image, label):\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray\"\n",
    "                                \"[eg: data readed by cv2.imread()].\\n\"))\n",
    "        if len(image.shape) > 3 or len(image.shape) < 2:\n",
    "            raise (RuntimeError(\n",
    "                \"segtransform.ToTensor() only handle np.ndarray with 3 dims or 2 dims.\\n\"))\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.expand_dims(image, axis=2)\n",
    "\n",
    "        image = torch.from_numpy(image.transpose((2, 0, 1)))\n",
    "        if not isinstance(image, torch.FloatTensor):\n",
    "            image = image.float().div(255)\n",
    "        if label is not None:\n",
    "            if not isinstance(label, np.ndarray):\n",
    "                raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray\"\n",
    "                                    \"[eg: data readed by cv2.imread()].\\n\"))\n",
    "            if not len(label.shape) == 2:\n",
    "                raise (RuntimeError(\n",
    "                    \"segtransform.ToTensor() only handle np.ndarray labellabel with 2 dims.\\n\"))\n",
    "            label = torch.from_numpy(label)\n",
    "            if not isinstance(label, torch.LongTensor):\n",
    "                label = label.long()\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    # Normalize tensor with mean and standard deviation along channel: channel = (channel - mean) / std\n",
    "    def __init__(self, mean, std=None):\n",
    "        if std is None:\n",
    "            assert len(mean) > 0\n",
    "        else:\n",
    "            assert len(mean) == len(std)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        if self.std is None:\n",
    "            for t, m in zip(image, self.mean):\n",
    "                t.sub_(m)\n",
    "        else:\n",
    "            for t, m, s in zip(image, self.mean, self.std):\n",
    "                t.sub_(m).div_(s)\n",
    "        if label is not None:\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    # Resize the input to the given size, 'size' is a 2-element tuple or list in the order of (h, w).\n",
    "    def __init__(self, size, padding=None):\n",
    "        self.size = size\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "\n",
    "        def find_new_hw(ori_h, ori_w, test_size):\n",
    "            if ori_h >= ori_w:\n",
    "                ratio = test_size * 1.0 / ori_h\n",
    "                new_h = test_size                 # test_size is target_size\n",
    "                new_w = int(ori_w * ratio)\n",
    "            elif ori_w > ori_h:\n",
    "                ratio = test_size * 1.0 / ori_w\n",
    "                new_h = int(ori_h * ratio)\n",
    "                new_w = test_size\n",
    "\n",
    "            if new_h % 8 != 0:\n",
    "                new_h = (int(new_h / 8)) * 8   # 为什么新的长宽是8的倍数\n",
    "            else:\n",
    "                new_h = new_h\n",
    "            if new_w % 8 != 0:\n",
    "                new_w = (int(new_w / 8)) * 8\n",
    "            else:\n",
    "                new_w = new_w\n",
    "            return new_h, new_w\n",
    "\n",
    "        # Step 1: resize while keeping the h/w ratio. The largest side (i.e height or width) is reduced to $size.\n",
    "        #                                             The other is reduced accordingly\n",
    "        test_size = self.size\n",
    "        new_h, new_w = find_new_hw(image.shape[0], image.shape[1], test_size)\n",
    "\n",
    "        image_crop = cv2.resize(image, dsize=(int(new_w), int(new_h)),\n",
    "                                interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # Step 2: Pad wtih 0 whatever needs to be padded to get a ($size, $size) image\n",
    "        back_crop = np.zeros((test_size, test_size, 3))\n",
    "        if self.padding:\n",
    "            back_crop[:, :, 0] = self.padding[0]\n",
    "            back_crop[:, :, 1] = self.padding[1]\n",
    "            back_crop[:, :, 2] = self.padding[2]\n",
    "        back_crop[:new_h, :new_w, :] = image_crop\n",
    "        image = back_crop\n",
    "\n",
    "        # Step 3: Do the same for the label (the padding is 255)\n",
    "        if label is not None:\n",
    "            s_mask = label\n",
    "            new_h, new_w = find_new_hw(\n",
    "                s_mask.shape[0], s_mask.shape[1], test_size)\n",
    "            s_mask = cv2.resize(s_mask.astype(np.float32), dsize=(int(new_w), int(new_h)),\n",
    "                                interpolation=cv2.INTER_NEAREST)\n",
    "            back_crop_s_mask = np.ones((test_size, test_size)) * 255\n",
    "            back_crop_s_mask[:new_h, :new_w] = s_mask\n",
    "            label = back_crop_s_mask\n",
    "\n",
    "            return image, label\n",
    "        else:\n",
    "            return image, new_h, new_w\n",
    "\n",
    "\n",
    "class Resize_np(object):\n",
    "    # Resize the input to the given size, 'size' is a 2-element tuple or list in the order of (h, w).\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, int):\n",
    "            self.size = (size, size)\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "\n",
    "        # resize image\n",
    "        # F.resize(image, self.size, self.interpolation)\n",
    "        image = cv2.resize(image, dsize=self.size,\n",
    "                           interpolation=cv2.INTER_LINEAR)\n",
    "        image = image.astype(np.int)\n",
    "        # resize the label\n",
    "        label = cv2.resize(label.astype(np.float32),\n",
    "                           dsize=self.size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class RandScale(object):\n",
    "    # Randomly resize image & label with scale factor in [scale_min, scale_max]\n",
    "    def __init__(self, scale, aspect_ratio=None, fixed_size=None, padding=None):\n",
    "        assert (isinstance(scale, collections.Iterable) and len(scale) == 2)\n",
    "        if isinstance(scale[0], numbers.Number) \\\n",
    "                and isinstance(scale[1], numbers.Number) \\\n",
    "                and 0 < scale[0] < scale[1]:\n",
    "            self.scale = scale               # scale = (0.5, 1.5)\n",
    "        else:\n",
    "            raise (RuntimeError(\"segRandScale() scale param error.\\n\"))\n",
    "\n",
    "        if aspect_ratio is None:\n",
    "            self.aspect_ratio = aspect_ratio\n",
    "        elif isinstance(aspect_ratio, collections.Iterable) \\\n",
    "                and len(aspect_ratio) == 2 \\\n",
    "                and isinstance(aspect_ratio[0], numbers.Number) \\\n",
    "                and isinstance(aspect_ratio[1], numbers.Number) \\\n",
    "                and 0 < aspect_ratio[0] < aspect_ratio[1]:\n",
    "            self.aspect_ratio = aspect_ratio\n",
    "        else:\n",
    "            raise (RuntimeError(\"segRandScale() aspect_ratio param error.\\n\"))\n",
    "\n",
    "        self.fixed_size, self.padding = fixed_size, padding\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        # 从 scale[0] 到 scale[1]随机选取一个scale\n",
    "        temp_scale = self.scale[0] + \\\n",
    "            (self.scale[1] - self.scale[0]) * random.random()\n",
    "        temp_aspect_ratio = 1.0\n",
    "        if self.aspect_ratio is not None:\n",
    "            temp_aspect_ratio = self.aspect_ratio[0] + (\n",
    "                self.aspect_ratio[1] - self.aspect_ratio[0]) * random.random()\n",
    "            temp_aspect_ratio = math.sqrt(temp_aspect_ratio)\n",
    "        scale_factor_x = temp_scale * temp_aspect_ratio\n",
    "        scale_factor_y = temp_scale / temp_aspect_ratio\n",
    "        image = cv2.resize(image, None, fx=scale_factor_x, fy=scale_factor_y,\n",
    "                           interpolation=cv2.INTER_LINEAR)\n",
    "        label = cv2.resize(label, None, fx=scale_factor_x, fy=scale_factor_y,\n",
    "                           interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        if self.fixed_size is not None and self.fixed_size > 0:\n",
    "            new_h, new_w, _ = image.shape\n",
    "\n",
    "            back_crop = np.zeros((self.fixed_size, self.fixed_size, 3))\n",
    "            if self.padding:\n",
    "                back_crop[:, :, 0] = self.padding[0]\n",
    "                back_crop[:, :, 1] = self.padding[1]\n",
    "                back_crop[:, :, 2] = self.padding[2]\n",
    "            back_crop[:new_h, :new_w, :] = image\n",
    "            image = back_crop\n",
    "\n",
    "            back_crop_mask = np.ones((self.fixed_size, self.fixed_size)) * 255\n",
    "            back_crop_mask[:new_h, :new_w] = label\n",
    "            label = back_crop_mask\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class Crop(object):\n",
    "    \"\"\"Crops the given ndarray image (H*W*C or H*W).\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "        int instead of sequence like (h, w), a square crop (size, size) is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, crop_type='center', padding=None, ignore_label=255):\n",
    "        if isinstance(size, int):\n",
    "            self.crop_h = size\n",
    "            self.crop_w = size\n",
    "        elif isinstance(size, collections.Iterable) and len(size) == 2 \\\n",
    "                and isinstance(size[0], int) and isinstance(size[1], int) \\\n",
    "                and size[0] > 0 and size[1] > 0:\n",
    "            self.crop_h = size[0]\n",
    "            self.crop_w = size[1]\n",
    "        else:\n",
    "            raise (RuntimeError(\"crop size error.\\n\"))\n",
    "        if crop_type == 'center' or crop_type == 'rand':\n",
    "            self.crop_type = crop_type\n",
    "        else:\n",
    "            raise (RuntimeError(\"crop type error: rand | center\\n\"))\n",
    "        if padding is None:\n",
    "            self.padding = padding\n",
    "        elif isinstance(padding, list):\n",
    "            if all(isinstance(i, numbers.Number) for i in padding):\n",
    "                self.padding = padding\n",
    "            else:\n",
    "                raise (RuntimeError(\"padding in Crop() should be a number list\\n\"))\n",
    "            if len(padding) != 3:\n",
    "                raise (RuntimeError(\"padding channel is not equal with 3\\n\"))\n",
    "        else:\n",
    "            raise (RuntimeError(\"padding in Crop() should be a number list\\n\"))\n",
    "        if isinstance(ignore_label, int):\n",
    "            self.ignore_label = ignore_label\n",
    "        else:\n",
    "            raise (RuntimeError(\"ignore_label should be an integer number\\n\"))\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        h, w = image.shape[:2]\n",
    "        pad_h = max(self.crop_h - h, 0)\n",
    "        pad_w = max(self.crop_w - w, 0)\n",
    "        pad_h_half = int(pad_h / 2)\n",
    "        pad_w_half = int(pad_w / 2)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            if self.padding is None:\n",
    "                raise (RuntimeError(\n",
    "                    \"segtransform.Crop() need padding while padding argument is None\\n\"))\n",
    "            image = cv2.copyMakeBorder(image, pad_h_half, pad_h - pad_h_half, pad_w_half,\n",
    "                                       pad_w - pad_w_half, cv2.BORDER_CONSTANT, value=self.padding)\n",
    "            # image = np.zeros(3,)\n",
    "            if label is not None:\n",
    "                label = cv2.copyMakeBorder(label, pad_h_half, pad_h - pad_h_half, pad_w_half,\n",
    "                                           pad_w - pad_w_half, cv2.BORDER_CONSTANT, value=self.ignore_label)\n",
    "        h, w = image.shape[:2]\n",
    "        if self.crop_type == 'rand':\n",
    "            h_off = random.randint(0, h - self.crop_h)\n",
    "            w_off = random.randint(0, w - self.crop_w)\n",
    "        else:\n",
    "            h_off = int((h - self.crop_h) / 2)\n",
    "            w_off = int((w - self.crop_w) / 2)\n",
    "        image = image[h_off:h_off+self.crop_h, w_off:w_off+self.crop_w]\n",
    "        image = image.astype(np.int)\n",
    "        if label is not None:\n",
    "            label = label[h_off:h_off+self.crop_h, w_off:w_off+self.crop_w]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "class FitCrop(object):\n",
    "    \"\"\"Crops the given ndarray image (H*W*C or H*W).\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "        int instead of sequence like (h, w), a square crop (size, size) is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=2, multi=False):\n",
    "        self.k = k  # whether to crop at 1/2 or 1/3,  if fg is very small portion, will cutoff bigger area\n",
    "        self.multi = multi  # whether to return multiple cropped image\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        label_binary = label.copy()\n",
    "        label_binary[label_binary == 255] = 0\n",
    "        _, labels = cv2.connectedComponents(label_binary)  # labels 为联通域 的 idx\n",
    "\n",
    "        freq = np.bincount(labels.flatten())\n",
    "        freq[0] = 0\n",
    "        obj_idx = np.argmax(freq)      # id for 最大联通域\n",
    "        pxl_cnt = freq[obj_idx]\n",
    "        h0, h1, w0, w1 = self.get_coord(labels, obj_idx, h, w)\n",
    "        image = image[h0:h1, w0:w1]\n",
    "        label = label[h0:h1, w0:w1]\n",
    "\n",
    "        if self.multi and len(freq) >= 3:\n",
    "            freq[obj_idx] = 0\n",
    "            obj_idx2 = np.argmax(freq)\n",
    "            pxl_cnt2 = freq[obj_idx2]\n",
    "\n",
    "            if pxl_cnt2 / pxl_cnt >= 0.3:\n",
    "                h0, h1, w0, w1 = self.get_coord(labels, obj_idx2, h, w)\n",
    "                image2 = image[h0:h1, w0:w1]\n",
    "                label2 = label[h0:h1, w0:w1]\n",
    "\n",
    "                return image, label, image2, label2\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_coord(self, labels, obj_idx, h, w):\n",
    "        mask_pos = np.where(labels == obj_idx)\n",
    "        min_h, max_h, min_w, max_w = np.min(mask_pos[0]), np.max(\n",
    "            mask_pos[0]), np.min(mask_pos[1]), np.max(mask_pos[1])\n",
    "\n",
    "        h0, h1 = min_h // self.k, h - (h - max_h) // self.k\n",
    "        w0, w1 = min_w // self.k, w - (w - max_w) // self.k\n",
    "\n",
    "        if (h1 - h0) / (w1 - w0) <= 0.7:  # height too small\n",
    "            if h0 <= h - h1:\n",
    "                h0 = 0\n",
    "            else:\n",
    "                h1 = h\n",
    "        elif (h1 - h0) / (w1 - w0) >= 1.5:  # width too small\n",
    "            if w0 <= w - w1:\n",
    "                w0 = 0\n",
    "            else:\n",
    "                w1 = w\n",
    "        return h0, h1, w0, w1\n",
    "\n",
    "\n",
    "class RandRotate(object):\n",
    "    # Randomly rotate image & label with rotate factor in [rotate_min, rotate_max]\n",
    "    def __init__(self, rotate, padding, ignore_label=255, p=0.5):\n",
    "        assert (isinstance(rotate, collections.Iterable) and len(rotate) == 2)\n",
    "        if isinstance(rotate[0], numbers.Number) and isinstance(rotate[1], numbers.Number) \\\n",
    "                and rotate[0] < rotate[1]:\n",
    "            self.rotate = rotate\n",
    "        else:\n",
    "            raise (RuntimeError(\"segtransform.RandRotate() scale param error.\\n\"))\n",
    "        assert padding is not None\n",
    "        assert isinstance(padding, list) and len(padding) == 3\n",
    "        if all(isinstance(i, numbers.Number) for i in padding):\n",
    "            self.padding = padding\n",
    "        else:\n",
    "            raise (RuntimeError(\"padding in RandRotate() should be a number list\\n\"))\n",
    "        assert isinstance(ignore_label, int)\n",
    "        self.ignore_label = ignore_label\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        if random.random() < self.p:\n",
    "            angle = self.rotate[0] + \\\n",
    "                (self.rotate[1] - self.rotate[0]) * random.random()\n",
    "            h, w = label.shape\n",
    "            matrix = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1)\n",
    "            image = cv2.warpAffine(image, matrix, (w, h), flags=cv2.INTER_LINEAR,\n",
    "                                   borderMode=cv2.BORDER_CONSTANT, borderValue=self.padding)\n",
    "            label = cv2.warpAffine(label, matrix, (w, h), flags=cv2.INTER_NEAREST,\n",
    "                                   borderMode=cv2.BORDER_CONSTANT, borderValue=self.ignore_label)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        if random.random() < self.p:\n",
    "            image = cv2.flip(image, 1)\n",
    "            label = cv2.flip(label, 1)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        if random.random() < self.p:\n",
    "            image = cv2.flip(image, 0)\n",
    "            label = cv2.flip(label, 0)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class RandomGaussianBlur(object):\n",
    "    def __init__(self, radius=5):\n",
    "        self.radius = radius\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        if random.random() < 0.5:\n",
    "            image = cv2.GaussianBlur(image, (self.radius, self.radius), 0)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class ColorJitter(object):\n",
    "    def __init__(self, cj_type='b'):\n",
    "        self.cj_type = cj_type\n",
    "\n",
    "    def __call__(self, img, label):\n",
    "        '''\n",
    "        ### Different Color Jitter ###\n",
    "        img: image\n",
    "        cj_type: {b: brightness, s: saturation, c: constast}\n",
    "        '''\n",
    "        if self.cj_type == \"b\":\n",
    "            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "            # Hue, Saturation, and Value (Brightness)\n",
    "            h, s, v = cv2.split(hsv)\n",
    "            value = 35 if np.mean(v) <= 125 else -35\n",
    "            if value >= 0:\n",
    "                lim = 255 - value\n",
    "                v[v > lim] = 255\n",
    "                v[v <= lim] += value\n",
    "            else:\n",
    "                lim = np.absolute(value)\n",
    "                v[v < lim] = 0\n",
    "                v[v >= lim] -= np.absolute(value)\n",
    "\n",
    "            final_hsv = cv2.merge((h, s, v))\n",
    "            img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        elif self.cj_type == \"s\":\n",
    "            # value = random.randint(-50, 50)\n",
    "            value = np.random.choice(np.array([0.5, 0.75, 1.25, 1.5]))\n",
    "            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "            h, s, v = cv2.split(hsv)\n",
    "            s *= value\n",
    "\n",
    "            final_hsv = cv2.merge((h, s, v))\n",
    "            img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        elif self.cj_type == \"c\":\n",
    "            brightness = 10\n",
    "            contrast = random.randint(40, 100)\n",
    "            dummy = np.int16(img)\n",
    "            dummy = dummy * (contrast / 127 + 1) - contrast + brightness\n",
    "            img = np.clip(dummy, 0, 255)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class ColorAug(object):\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        # [max(0, 1 - brightness), 1 + brightness]\n",
    "        self.brightness = brightness\n",
    "        # [max(0, 1 - contrast), 1 + contrast]\n",
    "        self.contrast = contrast\n",
    "        # [max(0, 1 - saturation), 1 + saturation]\n",
    "        self.saturation = saturation\n",
    "        self.hue = hue                                 # [-hue, hue]\n",
    "        self.gitter = transforms.ColorJitter(\n",
    "            self.brightness, self.contrast, self.saturation, self.hue)\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        image = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "        image = self.gitter(image)\n",
    "        image = np.array(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class Contrast(object):\n",
    "    def __init__(self, v=0.9, max_v=0.05, bias=0):\n",
    "        self.v = _float_parameter(v, max_v) + bias\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        image = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "        return PIL.ImageEnhance.Contrast(image).enhance(self.v), label\n",
    "\n",
    "\n",
    "class Brightness(object):\n",
    "    def __init__(self, v=1.8, max_v=0.1, bias=0):\n",
    "        self.v = _float_parameter(v, max_v) + bias\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        image = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "        return PIL.ImageEnhance.Brightness(image).enhance(self.v), label\n",
    "\n",
    "\n",
    "class Sharpness(object):\n",
    "    def __init__(self, v=0.9, max_v=0.05, bias=0):\n",
    "        self.v = _float_parameter(v, max_v) + bias\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        image = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "        return PIL.ImageEnhance.Sharpness(image).enhance(self.v), label\n",
    "\n",
    "\n",
    "class AutoContrast(object):\n",
    "    def __call__(self, image, label):\n",
    "        image = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "        return PIL.ImageOps.autocontrast(image), label\n",
    "\n",
    "\n",
    "def _float_parameter(v, max_v):\n",
    "    return float(v) * max_v / PARAMETER_MAX\n",
    "\n",
    "\n",
    "class RGB2BGR(object):\n",
    "    # Converts image from RGB order to BGR order, for model initialized from Caffe\n",
    "    def __call__(self, image, label):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class BGR2RGB(object):\n",
    "    # Converts image from BGR order to RGB order, for model initialized from Pytorch\n",
    "    def __call__(self, image, label):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding:utf-8\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict, Iterable, List, Tuple, TypeVar\n",
    "\n",
    "\n",
    "IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm']\n",
    "\n",
    "A = TypeVar(\"A\")\n",
    "B = TypeVar(\"B\")\n",
    "\n",
    "\n",
    "def mmap_(fn: Callable[[A], B], iter: Iterable[A]) -> List[B]:\n",
    "    return Pool().map(fn, iter)\n",
    "\n",
    "\n",
    "def is_image_file(filename: str) -> bool:\n",
    "    filename_lower = filename.lower()\n",
    "    return any(filename_lower.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def make_dataset(\n",
    "        data_root: str,\n",
    "        data_list: str,\n",
    "        class_list: List[int]\n",
    ") -> Tuple[List[Tuple[str, str]], Dict[int, List[Tuple[str, str]]]]:\n",
    "\n",
    "    if not os.path.isfile(data_list):\n",
    "        raise (RuntimeError(\"Image list file do not exist: \" + data_list + \"\\n\"))\n",
    "    '''\n",
    "        Recovers all tupples (img_path, label_path) relevant to the current experiments (class_list\n",
    "        is used as filter)\n",
    "\n",
    "        input:\n",
    "            data_root : Path to the data directory\n",
    "            data_list : Path to the .txt file that contain the train/test split of images\n",
    "            class_list: List of classes to keep\n",
    "        returns:\n",
    "            image_label_list: List of (img_path, label_path) that contain at least 1 object of a class\n",
    "                              in class_list\n",
    "            class_file_dict: Dict of all (img_path, label_path that contain at least 1 object of a class\n",
    "                              in class_list, grouped by classes.\n",
    "    '''\n",
    "    image_label_list: List[Tuple[str, str]] = []\n",
    "    list_read = open(data_list).readlines()\n",
    "\n",
    "    print(f\"Processing data for {class_list}\")\n",
    "    class_file_dict: Dict[int, List[Tuple[str, str]]] = defaultdict(list)\n",
    "\n",
    "    process_partial = partial(\n",
    "        process_image, data_root=data_root, class_list=class_list)\n",
    "\n",
    "    for sublist, subdict in mmap_(process_partial, tqdm(list_read)):\n",
    "        image_label_list += sublist\n",
    "\n",
    "        for (k, v) in subdict.items():\n",
    "            class_file_dict[k] += v\n",
    "\n",
    "    return image_label_list, class_file_dict\n",
    "    # image_label_list：list of 所有包含的图片 [(image_filename, label_filename)], class_file_dict：cls_id->相应图片list\n",
    "\n",
    "\n",
    "def process_image(\n",
    "        line: str,\n",
    "        data_root: str,\n",
    "        class_list: List\n",
    ") -> Tuple[List, Dict]:\n",
    "    '''\n",
    "        Reads and parses a line corresponding to 1 file\n",
    "\n",
    "        input:\n",
    "            line : A line corresponding to 1 file, in the format path_to_image.jpg path_to_image.png\n",
    "            data_root : Path to the data directory\n",
    "            class_list: List of classes to keep\n",
    "\n",
    "    '''\n",
    "    line = line.strip()\n",
    "    line_split = line.split(' ')\n",
    "    image_name = os.path.join(data_root, line_split[0])   # image_file\n",
    "    label_name = os.path.join(data_root, line_split[1])   # label_file\n",
    "    item: Tuple[str, str] = (image_name, label_name)\n",
    "    label = cv2.imread(label_name, cv2.IMREAD_GRAYSCALE)\n",
    "    label_class = np.unique(label).tolist()                # 当前图片的所有 category\n",
    "\n",
    "    if 0 in label_class:\n",
    "        label_class.remove(0)\n",
    "    if 255 in label_class:\n",
    "        label_class.remove(255)\n",
    "    for label_class_ in label_class:\n",
    "        assert label_class_ in list(range(1, 81)), label_class_\n",
    "\n",
    "    c: int\n",
    "    new_label_class = []                                # 选取符合条件, 在meta train中的label\n",
    "    for c in label_class:\n",
    "        if c in class_list:                             # 保证cls在当前图片中占有一定的比重\n",
    "            tmp_label = np.zeros_like(label)\n",
    "            target_pix = np.where(label == c)          # 返回 row idx 和 col idx\n",
    "            tmp_label[target_pix[0], target_pix[1]] = 1\n",
    "            if tmp_label.sum() >= 2 * 32 * 32:\n",
    "                new_label_class.append(c)\n",
    "\n",
    "    label_class = new_label_class    # 筛选了当前图片中满足条件（最少有32*32*2个pixel）的所有cls\n",
    "\n",
    "    image_label_list: List[Tuple[str, str]] = []\n",
    "    class_file_dict: Dict[int, List[Tuple[str, str]]] = defaultdict(list)\n",
    "\n",
    "    if len(label_class) > 0:\n",
    "        image_label_list.append(item)  # item包含 image filename & label filname\n",
    "\n",
    "        for c in label_class:\n",
    "            assert c in class_list\n",
    "            class_file_dict[c].append(item)\n",
    "\n",
    "    return image_label_list, class_file_dict\n",
    "    # image_label_list：list of 所有包含的图片, class_file_dict：cls_id->相应图片list: 只针对当前这张图片！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding:utf-8\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "classId2className = {'coco': {\n",
    "    1: 'person',\n",
    "    2: 'bicycle',\n",
    "    3: 'car',\n",
    "    4: 'motorcycle',\n",
    "    5: 'airplane',\n",
    "    6: 'bus',\n",
    "    7: 'train',\n",
    "    8: 'truck',\n",
    "    9: 'boat',\n",
    "    10: 'traffic light',\n",
    "    11: 'fire hydrant',\n",
    "    12: 'stop sign',\n",
    "    13: 'parking meter',\n",
    "    14: 'bench',\n",
    "    15: 'bird',\n",
    "    16: 'cat',\n",
    "    17: 'dog',\n",
    "    18: 'horse',\n",
    "    19: 'sheep',\n",
    "    20: 'cow',\n",
    "    21: 'elephant',\n",
    "    22: 'bear',\n",
    "    23: 'zebra',\n",
    "    24: 'giraffe',\n",
    "    25: 'backpack',\n",
    "    26: 'umbrella',\n",
    "    27: 'handbag',\n",
    "    28: 'tie',\n",
    "    29: 'suitcase',\n",
    "    30: 'frisbee',\n",
    "    31: 'skis',\n",
    "    32: 'snowboard',\n",
    "    33: 'sports ball',\n",
    "    34: 'kite',\n",
    "    35: 'baseball bat',\n",
    "    36: 'baseball glove',\n",
    "    37: 'skateboard',\n",
    "    38: 'surfboard',\n",
    "    39: 'tennis racket',\n",
    "    40: 'bottle',\n",
    "    41: 'wine glass',\n",
    "    42: 'cup',\n",
    "    43: 'fork',\n",
    "    44: 'knife',\n",
    "    45: 'spoon',\n",
    "    46: 'bowl',\n",
    "    47: 'banana',\n",
    "    48: 'apple',\n",
    "    49: 'sandwich',\n",
    "    50: 'orange',\n",
    "    51: 'broccoli',\n",
    "    52: 'carrot',\n",
    "    53: 'hot dog',\n",
    "    54: 'pizza',\n",
    "    55: 'donut',\n",
    "    56: 'cake',\n",
    "    57: 'chair',\n",
    "    58: 'sofa',\n",
    "    59: 'pottedplant',\n",
    "    60: 'bed',\n",
    "    61: 'diningtable',\n",
    "    62: 'toilet',\n",
    "    63: 'tv',\n",
    "    64: 'laptop',\n",
    "    65: 'mouse',\n",
    "    66: 'remote',\n",
    "    67: 'keyboard',\n",
    "    68: 'cell phone',\n",
    "    69: 'microwave',\n",
    "    70: 'oven',\n",
    "    71: 'toaster',\n",
    "    72: 'sink',\n",
    "    73: 'refrigerator',\n",
    "    74: 'book',\n",
    "    75: 'clock',\n",
    "    76: 'vase',\n",
    "    77: 'scissors',\n",
    "    78: 'teddy bear',\n",
    "    79: 'hair drier',\n",
    "    80: 'toothbrush'},\n",
    "\n",
    "    'pascal': {\n",
    "    1: 'airplane',   # 0.14\n",
    "    2: 'bicycle',    # 0.07\n",
    "    3: 'bird',       # 0.13\n",
    "    4: 'boat',       # 0.12\n",
    "    5: 'bottle',     # 0.15\n",
    "    6: 'bus',        # 0.35\n",
    "    7: 'cat',        # 0.20\n",
    "    8: 'car',        # 0.26\n",
    "    9: 'chair',      # 0.10\n",
    "    10: 'cow',       # 0.24\n",
    "    11: 'diningtable',  # 0.22\n",
    "    12: 'dog',         # 0.23\n",
    "    13: 'horse',       # 0.21\n",
    "    14: 'motorcycle',  # 0.22\n",
    "    15: 'person',      # 0.20\n",
    "    16: 'pottedplant',  # 0.11\n",
    "    17: 'sheep',       # 0.19\n",
    "    18: 'sofa',        # 0.23\n",
    "    19: 'train',       # 0.27\n",
    "    20: 'tv'           # 0.14\n",
    "}\n",
    "}\n",
    "\n",
    "className2classId = defaultdict(dict)\n",
    "for dataset in classId2className:\n",
    "    for id in classId2className[dataset]:\n",
    "        className2classId[dataset][classId2className[dataset][id]] = id\n",
    "\n",
    "\n",
    "def get_split_classes(args: argparse.Namespace) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Returns the split of classes for Pascal-5i and Coco-20i\n",
    "    inputs:\n",
    "        args\n",
    "\n",
    "    returns :\n",
    "         split_classes : Dict.\n",
    "                         split_classes['coco'][0]['train'] = training classes in fold 0 of Coco-20i\n",
    "    \"\"\"\n",
    "    split_classes = {'coco': defaultdict(dict), 'pascal': defaultdict(dict)}\n",
    "\n",
    "    # =============== COCO ===================\n",
    "    name = 'coco'\n",
    "    class_list = list(range(1, 81))\n",
    "    # key: coco -> -1 -> val  \"split -1 包含所有的class\"\n",
    "    split_classes[name][-1]['val'] = class_list\n",
    "    if args.use_split_coco:\n",
    "        vals_lists = [list(range(1, 78, 4)), list(range(2, 79, 4)),\n",
    "                      list(range(3, 80, 4)), list(range(4, 81, 4))]\n",
    "        # vals_lists = [[5, 2, 15, 9, 40], [6, 3, 16, 57, 20],\n",
    "        #               [61, 17, 18, 4, 1], [59, 19, 58, 7, 63]]\n",
    "        for i, val_list in enumerate(vals_lists):\n",
    "            split_classes[name][i]['val'] = val_list\n",
    "            split_classes[name][i]['train'] = list(\n",
    "                set(class_list) - set(val_list))\n",
    "\n",
    "    else:\n",
    "        class_list = list(range(1, 81))\n",
    "        vals_lists = [list(range(1, 21)), list(range(21, 41)),         # 共80个class,4个split.\n",
    "                      list(range(41, 61)), list(range(61, 81))]\n",
    "        for i, val_list in enumerate(vals_lists):\n",
    "            split_classes[name][i]['val'] = val_list\n",
    "            split_classes[name][i]['train'] = list(\n",
    "                set(class_list) - set(val_list))\n",
    "\n",
    "    # =============== Pascal ===================\n",
    "    name = 'pascal'\n",
    "    class_list = list(range(1, 21))\n",
    "    vals_lists = [list(range(1, 6)), list(range(6, 11)),\n",
    "                  list(range(11, 16)), list(range(16, 21))]\n",
    "    split_classes[name][-1]['val'] = class_list\n",
    "    for i, val_list in enumerate(vals_lists):\n",
    "        split_classes[name][i]['val'] = val_list\n",
    "        split_classes[name][i]['train'] = list(set(class_list) - set(val_list))\n",
    "\n",
    "    return split_classes\n",
    "\n",
    "\n",
    "def filter_classes(train_name: str,\n",
    "                   train_split: int,\n",
    "                   test_name: str,\n",
    "                   test_split: int,\n",
    "                   split_classes: Dict) -> List[int]:\n",
    "    \"\"\" Useful for domain shift experiments. Filters out classes that were seen\n",
    "        during  training (i.e in the train_name dataset) from the current list.\n",
    "\n",
    "    inputs:\n",
    "        train_name : 'coco' or 'pascal'\n",
    "        test_name : 'coco' or 'pascal'\n",
    "        train_split : In {0, 1, 2, 3}\n",
    "        test_split : In {0, 1, 2, 3, -1}. -1 represents \"all classes\" (the one used in our experiments)\n",
    "        split_classes: Dict of all classes used for each dataset and each split\n",
    "\n",
    "\n",
    "    returns :\n",
    "        kept_classes_id : Filtered list of class ids that will be used for testing\n",
    "    \"\"\"\n",
    "    print(f'INFO: {train_name} -> {test_name}')\n",
    "    print(f'INFO: {train_split} -> {test_split}')\n",
    "    print(\">> Start Filtering classes \")\n",
    "    seen_classes = [classId2className[train_name][c]\n",
    "                    for c in split_classes[train_name][train_split]['train']]  # 所有meta train cls name\n",
    "    # meta_test数据 cls id\n",
    "    initial_classes = split_classes[test_name][test_split]['val']\n",
    "    kept_classes_id = []\n",
    "    removed_classes = []\n",
    "    kept_classes_name = []\n",
    "    for c in initial_classes:\n",
    "        if classId2className[test_name][c] in seen_classes:\n",
    "            removed_classes.append(classId2className[test_name][c])\n",
    "        else:\n",
    "            kept_classes_id.append(c)\n",
    "            kept_classes_name.append(classId2className[test_name][c])\n",
    "    print(\">> Removed classes = {} \".format(removed_classes))\n",
    "    print(\">> Kept classes = {} \".format(kept_classes_name))\n",
    "    return kept_classes_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data.distributed import DistributedSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardData(Dataset):\n",
    "    def __init__(self, args: argparse.Namespace,\n",
    "                 transform: Compose,\n",
    "                 data_list_path: str,\n",
    "                 class_list: List[int],\n",
    "                 return_paths: bool):\n",
    "        # path to dataset directory\n",
    "        self.data_root = args.data_root\n",
    "        # dict of containing two dicts (pascal & coco), mapping the classes that corresponds to different dataset splits\n",
    "        self.class_list = class_list\n",
    "        # return a tuple with: 1, a List of all (image_path, label_path) Tuples; 2, a Dict mapping (image_path, label_path) Tuples for different classes\n",
    "        self.data_list, _ = make_dataset(\n",
    "            args.data_root, data_list_path, class_list)\n",
    "        # Composed transformation for dataset\n",
    "        self.transform = transform\n",
    "        # whether to return image&label path in __getitem__\n",
    "        self.return_paths = return_paths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data_list)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "\n",
    "            # read image and labels\n",
    "            image_path, label_path = self.data_list[index]\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = np.float32(image)\n",
    "            label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            # double check to make sure the shape matches\n",
    "            if image.shape[0] != label.shape[0] or image.shape[1] != label.shape[1]:\n",
    "                raise (RuntimeError(\"Query Image & label shape mismatch: \" +\n",
    "                       image_path + \" \" + label_path + \"\\n\"))\n",
    "\n",
    "            # remove unwanted label classes\n",
    "            label_class = np.unique(label).tolist()\n",
    "            if 0 in label_class:\n",
    "                label_class.remove(0)\n",
    "            if 255 in label_class:\n",
    "                label_class.remove(255)\n",
    "\n",
    "            new_label_class = []\n",
    "            undesired_class = []\n",
    "            for c in label_class:\n",
    "                if c in self.class_list:\n",
    "                    new_label_class.append(c)\n",
    "                else:\n",
    "                    undesired_class.append(c)\n",
    "            label_class = new_label_class\n",
    "            assert len(label_class) > 0\n",
    "\n",
    "            # background\n",
    "            new_label = np.zeros_like(label)\n",
    "            for lab in label_class:\n",
    "                indexes = np.where(label == lab)\n",
    "                new_label[indexes[0], indexes[1]] = self.class_list.index(\n",
    "                    lab) + 1       # Add 1 because class 0 is for bg\n",
    "            for lab in undesired_class:\n",
    "                indexes = np.where(label == lab)\n",
    "                new_label[indexes[0], indexes[1]] = 255\n",
    "\n",
    "            if self.transform is not None:\n",
    "                image, new_label = self.transform(image, new_label)\n",
    "            if self.return_paths:\n",
    "                return image, new_label, image_path, label_path\n",
    "            else:\n",
    "                return image, new_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicData(Dataset):\n",
    "    def __init__(self, mode_train: bool, dt_transform: Compose, class_list: List[int], args: argparse.Namespace):\n",
    "        self.args = args\n",
    "        self.shot = args.shot\n",
    "        self.class_list = class_list\n",
    "        self.transform = dt_transform\n",
    "        self.data_root = args.data_root\n",
    "        self.random_shot = args.random_shot\n",
    "        self.aug_type = args.get('aug_type', 0)\n",
    "        self.meta_aug = args.get('meta_aug', 0)\n",
    "        self.aug_th = args.get('aug_th', [0.15, 0.30])\n",
    "        self.padding = [v*255 for v in args.mean] if args.get('padding') == 'avg' else None\n",
    "\n",
    "        self.data_list, self.sub_class_file_list = make_dataset(args.data_root, args.train_list, self.class_list) if mode_train \\\n",
    "                                                    else make_dataset(args.data_root, args.val_list, self.class_list)\n",
    "\n",
    "        print(f\"Transformations applied: {self.transform.segtransform}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # ============================== Build Query ==============================\n",
    "        # ====== Read query image + Get suitable classes ======\n",
    "        image_path, label_path = self.data_list[index]\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = np.float32(image)\n",
    "        label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        assert image.shape[0]==label.shape[0] and image.shape[1] == label.shape[1], f\"Query & label shape mismatch: {image_path}, {label_path}\\n\"\n",
    "        \n",
    "        # ====== Retain only suitable class labels =====\n",
    "        original_label_class = np.unique(label).tolist()\n",
    "        if 0 in original_label_class:\n",
    "            original_label_class.remove(0)\n",
    "        if 255 in original_label_class:\n",
    "            original_label_class.remove(255)\n",
    "\n",
    "        label_class = []\n",
    "        for c in original_label_class:\n",
    "            if c in self.class_list:\n",
    "                label_class.append(c)\n",
    "        \n",
    "        assert len(label_class)>0, f\"No suitable class labels for at 'image path': {image_path}, 'label path': {label_path}\"\n",
    "\n",
    "        # ===== From classes in query image, chose one randomly =====\n",
    "        class_chosen = np.random.choice(label_class)\n",
    "        new_label = np.zeros_like(label)\n",
    "        ignore_pix = np.where(label == 255)\n",
    "        target_pix = np.where(label == class_chosen)\n",
    "        new_label[ignore_pix] = 255\n",
    "        new_label[target_pix] = 1\n",
    "        label = new_label\n",
    "\n",
    "        # 当前split 选取的class, 所对应的image/label path\n",
    "        file_list_class_chosen = self.sub_class_file_list[class_chosen]\n",
    "        num_file = len(file_list_class_chosen)\n",
    "\n",
    "        # ============================== Build Support ==============================\n",
    "        # First randomly choose indexes of support images\n",
    "        support_image_path_list = []\n",
    "        support_label_path_list = []\n",
    "        support_idx_list = []\n",
    "\n",
    "        shot = random.randint(1, self.shot) if self.random_shot else self.shot\n",
    "\n",
    "        for k in range(shot):\n",
    "            support_idx = random.randint(1, num_file) - 1\n",
    "\n",
    "            # init with this value to ensure going into the loop\n",
    "            support_image_path = image_path\n",
    "            support_label_path = label_path\n",
    "\n",
    "            # 排除 query img 并确保 support image(s) 没有重复\n",
    "            while(\n",
    "                (support_image_path == image_path and support_label_path == label_path)\n",
    "                or\n",
    "                (support_idx in support_idx_list)\n",
    "            ):                                                                     \n",
    "                support_idx = random.randint(1, num_file) - 1\n",
    "                support_image_path, support_label_path = file_list_class_chosen[support_idx]\n",
    "            support_idx_list.append(support_idx)\n",
    "            support_image_path_list.append(support_image_path)\n",
    "            support_label_path_list.append(support_label_path)\n",
    "\n",
    "        support_image_list = []\n",
    "        support_label_list = []\n",
    "        subcls_list = [self.class_list.index(class_chosen) + 1]\n",
    "\n",
    "        # Second, read support images and masks\n",
    "        for k in range(shot):\n",
    "            support_image_path = support_image_path_list[k]\n",
    "            support_label_path = support_label_path_list[k]\n",
    "            support_image = cv2.imread(support_image_path, cv2.IMREAD_COLOR)\n",
    "            support_image = cv2.cvtColor(support_image, cv2.COLOR_BGR2RGB)\n",
    "            support_image = np.float32(support_image)\n",
    "            support_label = cv2.imread(support_label_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            target_pix = np.where(support_label == class_chosen)\n",
    "            ignore_pix = np.where(support_label == 255)\n",
    "            support_label[:, :] = 0\n",
    "            support_label[target_pix[0], target_pix[1]] = 1\n",
    "            support_label[ignore_pix[0], ignore_pix[1]] = 255\n",
    "\n",
    "            assert support_image.shape[0] == support_label.shape[0] and support_image.shape[1] == support_label.shape[1],\\\n",
    "                f\"Support & label shape mismatch: support image path: {support_image_path}; support label path: {support_label_path}\\n\"\n",
    "            \n",
    "            support_image_list.append(support_image)\n",
    "            support_label_list.append(support_label)\n",
    "        assert len(support_label_list) == shot and len(support_image_list) == shot,\\\n",
    "            f\"Length of support image/label list is more than {shot} shot(s)\\n\"\n",
    "\n",
    "        # Original support images and labels\n",
    "        support_images = support_image_list.copy()\n",
    "        support_labels = support_label_list.copy()\n",
    "\n",
    "        support_images_without_norm = []\n",
    "\n",
    "        # ============================== Data Transform & Augmentations ==============================\n",
    "        if self.transform is not None:\n",
    "\n",
    "            qry_img, target = self.transform(image, label)    # transform query img\n",
    "            qry_img_without_norm, _ = \\\n",
    "                Compose(self.transform.segtransform[:-1])(image, label) # query img transformed without normalization\n",
    "\n",
    "            for k in range(shot):                             # transform support img\n",
    "                if self.meta_aug > 1:\n",
    "                    org_img, org_label = self.transform(support_image_list[k], support_label_list[k])  # flip and resize\n",
    "                    label_freq = np.bincount(support_label_list[k].flatten())\n",
    "                    fg_ratio = label_freq[1] / np.sum(label_freq)\n",
    "\n",
    "                    if self.aug_type == 0:\n",
    "                        new_img, new_label = self.get_aug_data0(\n",
    "                            fg_ratio, support_image_list[k], support_label_list[k])\n",
    "                    elif self.aug_type == 1:\n",
    "                        new_img, new_label = self.get_aug_data1(\n",
    "                            fg_ratio, support_image_list[k], support_label_list[k])\n",
    "                    elif self.aug_type == 3:\n",
    "                        new_img, new_label = self.get_aug_data3(\n",
    "                            fg_ratio, support_image_list[k], support_label_list[k])\n",
    "                    elif self.aug_type == 10:\n",
    "                        new_img, new_label = self.get_aug_data10(\n",
    "                            fg_ratio, support_image_list[k], support_label_list[k])\n",
    "                    # aug with ColorJitter\n",
    "                    elif self.aug_type == 4:\n",
    "                        new_img, new_label = self.get_aug_data4(\n",
    "                            fg_ratio, support_image_list[k], support_label_list[k], self.args)\n",
    "\n",
    "                    if new_img is not None:\n",
    "                        support_image_list[k] = torch.cat(\n",
    "                            [org_img.unsqueeze(0), new_img], dim=0)\n",
    "                        support_label_list[k] = torch.cat(\n",
    "                            [org_label.unsqueeze(0), new_label], dim=0)\n",
    "                    else:\n",
    "                        support_image_list[k], support_label_list[k] = org_img.unsqueeze(\n",
    "                            0), org_label.unsqueeze(0)\n",
    "\n",
    "                else:\n",
    "                    support_image_list[k], support_label_list[k] = self.transform(\n",
    "                        support_image_list[k], support_label_list[k])\n",
    "                    support_image_list[k] = support_image_list[k].unsqueeze(0)\n",
    "                    support_label_list[k] = support_label_list[k].unsqueeze(0)\n",
    "\n",
    "                spt_img_without_norm, spt_label_without_norm = Compose(self.transform.segtransform[:-1])(support_images[k], support_labels[k])\n",
    "                support_images_without_norm.append(spt_img_without_norm)\n",
    "                \n",
    "        # Reshape properly\n",
    "        spprt_imgs = torch.cat(support_image_list, 0)\n",
    "        spprt_labels = torch.cat(support_label_list, 0)\n",
    "        spprt_imgs_without_norm = torch.cat(support_images_without_norm, 0)\n",
    "\n",
    "        return qry_img, target, spprt_imgs, spprt_labels, subcls_list, \\\n",
    "            [support_image_path_list, support_labels, spprt_imgs_without_norm],\\\n",
    "            [image_path, label, qry_img_without_norm]\n",
    "        # subcls_list  返回的是 选取的class在所有meta train cls list 中的index+1/\n",
    "\n",
    "    def get_aug_data0(self, fg_ratio, support_image, support_label):\n",
    "        if fg_ratio <= self.aug_th[0]:\n",
    "            k = 2 if fg_ratio <= 0.03 else 3  # whether to crop at 1/2 or 1/3\n",
    "            meta_trans = Compose(\n",
    "                [FitCrop(k=k)] + self.transform.segtransform[-3:])\n",
    "        elif self.aug_th[0] < fg_ratio < self.aug_th[1]:\n",
    "            meta_trans = Compose(\n",
    "                [ColorJitter(cj_type='b')] + self.transform.segtransform[-3:])\n",
    "        else:\n",
    "            scale = 473 / max(support_label.shape) * 0.8\n",
    "            meta_trans = Compose([RandScale(scale=(\n",
    "                scale, scale + 0.1), fixed_size=473, padding=self.padding)] + self.transform.segtransform[-2:])\n",
    "        new_img, new_label = meta_trans(support_image, support_label)\n",
    "        return new_img.unsqueeze(0), new_label.unsqueeze(0)\n",
    "\n",
    "    # only size augmentation, no color augmentation\n",
    "    def get_aug_data10(self, fg_ratio, support_image, support_label):\n",
    "        if fg_ratio <= self.aug_th[0] or fg_ratio >= self.aug_th[1]:\n",
    "            if fg_ratio <= self.aug_th[0]:\n",
    "                k = 2 if fg_ratio <= 0.03 else 3  # whether to crop at 1/2 or 1/3\n",
    "                meta_trans = Compose(\n",
    "                    [FitCrop(k=k)] + self.transform.segtransform[-3:])\n",
    "            else:\n",
    "                scale = 473 / max(support_label.shape) * 0.7\n",
    "                meta_trans = Compose([RandScale(scale=(\n",
    "                    scale, scale + 0.1), fixed_size=473, padding=self.padding)] + self.transform.segtransform[-2:])\n",
    "            new_img, new_label = meta_trans(support_image, support_label)\n",
    "            return new_img.unsqueeze(0), new_label.unsqueeze(0)\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def get_aug_data1(self, fg_ratio, support_image, support_label):  # create two augmented data\n",
    "        scale = 473 / max(support_label.shape)\n",
    "\n",
    "        if fg_ratio <= self.aug_th[0]:  # 0.15\n",
    "            meta_trans1 = Compose(\n",
    "                [FitCrop(k=2)] + self.transform.segtransform[-3:])\n",
    "            meta_trans2 = Compose(\n",
    "                [FitCrop(k=3)] + self.transform.segtransform[-3:])\n",
    "        elif self.aug_th[0] < fg_ratio < self.aug_th[1]:\n",
    "            meta_trans1 = Compose(\n",
    "                [FitCrop(k=3)] + self.transform.segtransform[-3:])\n",
    "            meta_trans2 = Compose([RandScale(scale=(scale * 0.85, scale * 0.85 + 0.1),\n",
    "                                  fixed_size=473, padding=self.padding)] + self.transform.segtransform[-2:])\n",
    "        else:\n",
    "            meta_trans1 = Compose([RandScale(scale=(scale * 0.85, scale * 0.85 + 0.1),\n",
    "                                  fixed_size=473, padding=self.padding)] + self.transform.segtransform[-2:])\n",
    "            meta_trans2 = Compose([RandScale(scale=(scale * 0.85, scale * 0.85 + 0.1),\n",
    "                                  fixed_size=473, padding=self.padding)] + self.transform.segtransform[-2:])\n",
    "        new_img1, new_label1 = meta_trans1(support_image, support_label)\n",
    "        new_img2, new_label2 = meta_trans2(support_image, support_label)\n",
    "\n",
    "        new_imgs = torch.cat(\n",
    "            [new_img1.unsqueeze(0), new_img2.unsqueeze(0)], dim=0)\n",
    "        new_labels = torch.cat(\n",
    "            [new_label1.unsqueeze(0), new_label2.unsqueeze(0)], dim=0)\n",
    "        return new_imgs, new_labels\n",
    "\n",
    "    def get_aug_data2(self, fg_ratio, support_image, support_label):   # 最初的 data augmentation\n",
    "        if fg_ratio <= 0.15:\n",
    "            k = 2 if fg_ratio <= 0.05 else 3\n",
    "            meta_trans = Compose(\n",
    "                [FitCrop(k=k)] + self.transform.segtransform[-3:])\n",
    "        else:\n",
    "            meta_trans = Compose([RandomHorizontalFlip(\n",
    "                p=1.0)] + self.transform.segtransform[-3:])\n",
    "        new_img, new_label = meta_trans(support_image, support_label)\n",
    "        return new_img.unsqueeze(0), new_label.unsqueeze(0)\n",
    "\n",
    "    # base data augmentation: resize (with padding)\n",
    "    def get_aug_data3(self, fg_ratio, support_image, support_label):\n",
    "        if fg_ratio <= self.aug_th[0]:\n",
    "            k = 2 if fg_ratio <= 0.03 else 3  # whether to crop at 1/2 or 1/3\n",
    "            trans_crop = FitCrop(k=k, multi=True)\n",
    "            crop_out = trans_crop(support_image, support_label)\n",
    "            meta_trans = Compose(self.transform.segtransform[-3:])\n",
    "\n",
    "            new_img, new_label = meta_trans(crop_out[0], crop_out[1])\n",
    "            if len(crop_out) == 2:\n",
    "                return new_img.unsqueeze(0), new_label.unsqueeze(0)\n",
    "            elif len(crop_out) == 4:\n",
    "                new_img2, new_label2 = meta_trans(crop_out[2], crop_out[3])\n",
    "                return torch.cat([new_img.unsqueeze(0), new_img2.unsqueeze(0)], dim=0), torch.cat([new_label.unsqueeze(0), new_label2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        elif self.aug_th[0] < fg_ratio < self.aug_th[1]:\n",
    "            meta_trans = Compose(\n",
    "                [ColorJitter(cj_type='b')] + self.transform.segtransform[-3:])\n",
    "        else:\n",
    "            scale = 473 / max(support_label.shape) * 0.7\n",
    "            meta_trans = Compose([RandScale(scale=(\n",
    "                scale, scale + 0.1), fixed_size=473, padding=self.padding)] + self.transform.segtransform[-2:])\n",
    "        new_img, new_label = meta_trans(support_image, support_label)\n",
    "        return new_img.unsqueeze(0), new_label.unsqueeze(0)\n",
    "\n",
    "    def get_aug_data4(self, fg_ratio, support_image, support_label, args):\n",
    "        if fg_ratio <= self.aug_th[0]:\n",
    "            k = 2 if fg_ratio <= 0.03 else 3  # whether to crop at 1/2 or 1/3\n",
    "            meta_trans = Compose(\n",
    "                [FitCrop(k=k)] + self.transform.segtransform[-3:])\n",
    "        elif self.aug_th[0] < fg_ratio < self.aug_th[1]:\n",
    "            meta_trans = Compose([ColorAug(args.get('brightness', 0), args.get('contrast', 0), args.get(\n",
    "                'saturation', 0), args.get('hue', 0))] + self.transform.segtransform[-3:])\n",
    "        else:\n",
    "            scale = 473 / max(support_label.shape) * 0.8\n",
    "            meta_trans = Compose([RandScale(scale=(\n",
    "                scale, scale + 0.1), fixed_size=473, padding=self.padding)] + self.transform.segtransform[-2:])\n",
    "        new_img, new_label = meta_trans(support_image, support_label)\n",
    "        return new_img.unsqueeze(0), new_label.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_loader(args, episodic=True, return_path=False):\n",
    "    \"\"\"\n",
    "        Build the train loader. This is a episodic loader.\n",
    "    \"\"\"\n",
    "    assert args.train_split in [0, 1, 2, 3]\n",
    "    padding = [\n",
    "        v*255 for v in args.mean] if args.get('padding') == 'avg' else None\n",
    "    aug_dic = {\n",
    "        'randscale': RandScale([args.scale_min, args.scale_max]),\n",
    "        'randrotate': RandRotate(\n",
    "            [args.rot_min, args.rot_max],\n",
    "            padding=[0 for x in args.mean],\n",
    "            ignore_label=255\n",
    "        ),\n",
    "        'hor_flip': RandomHorizontalFlip(),\n",
    "        'vert_flip': RandomVerticalFlip(),\n",
    "        'crop': Crop(\n",
    "            [args.image_size, args.image_size], crop_type='rand',\n",
    "            padding=[0 for x in args.mean], ignore_label=255\n",
    "        ),\n",
    "        'resize': Resize(args.image_size),\n",
    "        'resize_np': Resize_np(size=(args.image_size, args.image_size)),\n",
    "        'color_aug': ColorAug(args.get('brightness', 0), args.get('contrast', 0), args.get('saturation', 0), args.get('hue', 0))\n",
    "    }\n",
    "\n",
    "    train_transform = [aug_dic[name] for name in args.augmentations]\n",
    "    train_transform += [ToTensor(), Normalize(mean=args.mean, std=args.std)]\n",
    "    train_transform = Compose(train_transform)\n",
    "\n",
    "    # 只用了 args.use_split_coco 这个参数， 返回coco和pascal所有4个split, dict of dict\n",
    "    split_classes = get_split_classes(args)\n",
    "    # list of all meta train class labels\n",
    "    class_list = split_classes[args.train_name][args.train_split]['train']\n",
    "\n",
    "    # ====== Build loader ======\n",
    "    if episodic:\n",
    "        train_data = EpisodicData(\n",
    "            mode_train=True, dt_transform=train_transform, class_list=class_list, args=args\n",
    "        )\n",
    "    else:\n",
    "        train_data = StandardData(transform=train_transform, class_list=class_list,\n",
    "                                  return_paths=return_path,  data_list_path=args.train_list,\n",
    "                                  args=args)\n",
    "\n",
    "    world_size = torch.distributed.get_world_size() if args.distributed else 1\n",
    "    train_sampler = DistributedSampler(\n",
    "        train_data) if args.distributed else None\n",
    "    batch_size = int(args.batch_size /\n",
    "                     world_size) if args.distributed else args.batch_size\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True)\n",
    "    return train_loader, train_sampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_loader(args, episodic=True, return_path=False):\n",
    "    \"\"\"\n",
    "        Build the episodic validation loader.\n",
    "    \"\"\"\n",
    "    assert args.test_split in [0, 1, 2, 3, -1, 'default']\n",
    "\n",
    "    val_trans = [ToTensor(), Normalize(mean=args.mean, std=args.std)]\n",
    "    if 'resize_np' in args.augmentations:                                                     # base aug 只有 resize\n",
    "        val_trans = [Resize_np(size=(args.image_size, args.image_size))] + val_trans\n",
    "    else:\n",
    "        padding = [v*255 for v in args.mean] if args.get('padding')=='avg' else None\n",
    "        val_trans = [Resize(args.image_size, padding=padding)] + val_trans\n",
    "    val_transform = Compose(val_trans)\n",
    "    val_sampler = None\n",
    "    split_classes = get_split_classes(args)     # 返回coco和pascal所有4个split, dict of dict\n",
    "\n",
    "    # ====== Filter out classes seen during training ======\n",
    "    if args.test_name == 'default':\n",
    "        test_name = args.train_name    # 'pascal'\n",
    "        test_split = args.train_split  # split 0\n",
    "    else:\n",
    "        test_name = args.test_name\n",
    "        test_split = args.test_split\n",
    "    class_list = filter_classes(args.train_name, args.train_split, test_name, test_split, split_classes)  # 只有cross domain时才有用\n",
    "\n",
    "    # ====== Build loader ======\n",
    "    if episodic:\n",
    "        val_data = EpisodicData(mode_train=False, dt_transform=val_transform, class_list=class_list, args=args)\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_data,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=args.workers,\n",
    "            pin_memory=True,\n",
    "            sampler=val_sampler)\n",
    "    else:\n",
    "        class_list = split_classes[args.train_name][args.train_split]['train']\n",
    "        val_data = StandardData(args, val_transform, class_list=class_list, return_paths=return_path, data_list_path=args.val_list)\n",
    "        val_loader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=args.workers,\n",
    "                                                 pin_memory=True,\n",
    "                                                 sampler=val_sampler)\n",
    "\n",
    "    return val_loader, val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Src.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB_param_noise: 0\n",
      "adapt_iter: 100\n",
      "agg: cat\n",
      "all_lr: l\n",
      "arch: resnet\n",
      "att_drop: 0.5\n",
      "att_type: 2\n",
      "att_wt: 0.3\n",
      "aug_th: [0.12, 0.25]\n",
      "aug_type: 0\n",
      "augmentations: ['resize']\n",
      "aux: False\n",
      "backbone_dim: 2048\n",
      "batch_size: 1\n",
      "batch_size_val: 1\n",
      "bins: [1, 2, 3, 6]\n",
      "bottleneck_dim: 512\n",
      "ckpt_path: checkpoints/\n",
      "ckpt_used: best\n",
      "cls_lr: 0.1\n",
      "cls_type: oooo\n",
      "conv4d: red\n",
      "data_root: /Users/nigel/Documents/Research-Git/Dataset/VOCdevkit/VOC2012\n",
      "dist: dot\n",
      "distributed: False\n",
      "dropout: 0.1\n",
      "encoder_dim: 512\n",
      "episodic: True\n",
      "epochs: 8\n",
      "exp_name: dot1_wt3_aug0_1125_att3\n",
      "gamma: 0.1\n",
      "gpus: [0]\n",
      "image_size: 473\n",
      "inner_loss: wce\n",
      "inner_loss_type: wt_ce\n",
      "layers: 50\n",
      "log_freq: 50\n",
      "log_iter: 1190\n",
      "loss_shot: avg\n",
      "loss_type: wt_dc\n",
      "lr_stepsize: 30\n",
      "m_scale: False\n",
      "main_optim: SGD\n",
      "manual_seed: 2021\n",
      "mean: [0.485, 0.456, 0.406]\n",
      "meta_aug: 1\n",
      "meta_loss: wdc\n",
      "milestones: [40, 70]\n",
      "mixup: False\n",
      "model_dir: model_ckpt\n",
      "momentum: 0.9\n",
      "n_runs: 1\n",
      "nesterov: True\n",
      "norm_feat: True\n",
      "norm_q: True\n",
      "norm_s: False\n",
      "num_classes_tr: 2\n",
      "num_classes_val: 5\n",
      "padding: avg\n",
      "padding_label: 255\n",
      "pretrained: False\n",
      "proj_drop: 0.5\n",
      "random_shot: False\n",
      "red_dim: False\n",
      "resume_weights: /Users/nigel/Documents/Research-Git/Vis/pretrained/pascal/split0/pspnet_resnet50/\n",
      "rmid: l34\n",
      "rot_max: 10\n",
      "rot_min: -10\n",
      "save_models: False\n",
      "scale_lr: 1.0\n",
      "scale_max: 3.0\n",
      "scale_min: 0.33\n",
      "scheduler: cosine\n",
      "shot: 1\n",
      "smoothing: True\n",
      "std: [0.229, 0.224, 0.225]\n",
      "temp: 20.0\n",
      "test_name: default\n",
      "test_num: 1000\n",
      "test_split: default\n",
      "train_list: lists/pascal/train.txt\n",
      "train_name: pascal\n",
      "train_split: 0\n",
      "trans_lr: 0.0015\n",
      "use_amp: False\n",
      "use_split_coco: False\n",
      "val_list: lists/pascal/val.txt\n",
      "wa: True\n",
      "weight_decay: 0.0001\n",
      "workers: 0\n",
      "wt_file: 1\n"
     ]
    }
   ],
   "source": [
    "cfg = load_cfg_from_cfg_file(CONFIG_PATH)\n",
    "cfg.data_root = DATA_ROOT\n",
    "cfg.distributed = False\n",
    "\n",
    "# ===== disable meta_aug, switch att_type to None, and change shot to 1 =====\n",
    "cfg.shot = SHOT\n",
    "cfg.meta_aug = META_AUG          # no additional augmentations will be applied during data loading if set to 1\n",
    "cfg.aug_type = AUG_TYPE          # default to aug type 0\n",
    "cfg.att_type = ATT_TYPE          # original, augmented, adaptive\n",
    "cfg.resume_weights = RESUME_PATH # path to resume_weight for backbone PSPNet\n",
    "\n",
    "# ===== PROTR MMN Related Configs =====\n",
    "cfg.norm_s = False\n",
    "cfg.norm_q = True\n",
    "cfg.inner_loss = 'wce'\n",
    "cfg.meta_loss = 'wdc'\n",
    "cfg.encoder_dim = 512\n",
    "\n",
    "if (SEARCH_MODE==1):\n",
    "    cfg.meta_aug = 1\n",
    "    cfg.att_type = 2\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.pspnet import get_model, PSPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading weight '/Users/nigel/Documents/Research-Git/Vis/pretrained/pascal/split0/pspnet_resnet50/without_bias.pth'\n",
      "=> loaded weight '/Users/nigel/Documents/Research-Git/Vis/pretrained/pascal/split0/pspnet_resnet50/without_bias.pth'\n"
     ]
    }
   ],
   "source": [
    "# psp backbone\n",
    "backbone = get_model(cfg)\n",
    "backbone_dict = backbone.state_dict()\n",
    "# load pspnet weight\n",
    "# ===================================================================\n",
    "if cfg.resume_weights:\n",
    "    if cfg.get('wt_file', 0) == 1:\n",
    "        fname = cfg.resume_weights + 'without_bias.pth'\n",
    "    else:\n",
    "        fname = cfg.resume_weights + 'with_bias.pth'\n",
    "    if os.path.isfile(fname):\n",
    "        print(\"=> loading weight '{}'\".format(fname))\n",
    "        pre_weight = torch.load(fname, map_location=torch.device('cpu'))['state_dict']\n",
    "        model_dict = backbone.state_dict()\n",
    "\n",
    "        for index, key in enumerate(model_dict.keys()):\n",
    "            if 'classifier' not in key and 'gamma' not in key:\n",
    "                if model_dict[key].shape == pre_weight[key].shape:\n",
    "                    model_dict[key] = pre_weight[key]\n",
    "                else:\n",
    "                    print( 'Pre-trained shape and model shape dismatch for {}'.format(key) )\n",
    "\n",
    "        backbone.load_state_dict(model_dict, strict=True)\n",
    "        print(\"=> loaded weight '{}'\".format(fname))\n",
    "    else:\n",
    "        print(\"=> no weight found at '{}'\".format(fname))\n",
    "# ===================================================================\n",
    "\n",
    "# if cfg.resume_weights:\n",
    "#     lines = []\n",
    "#     cfg.resume_weight = f'pretrained/{cfg.train_name}/split{cfg.train_split}/pspnet_resnet{cfg.layers}/best.pth'\n",
    "#     if os.path.isfile(cfg.resume_weight):\n",
    "#         lines.append(f'==> loading backbone weight from: {cfg.resume_weight}')\n",
    "#         pre_dict, cur_dict = torch.load(cfg.resume_weight, map_location=torch.device('cpu'))['state_dict'], backbone.state_dict()\n",
    "#         for key1, key2 in zip(pre_dict.keys(), cur_dict.keys()):\n",
    "#             if pre_dict[key1].shape != cur_dict[key2].shape:\n",
    "#                 lines.append(f'Pre-trained {key1} shape and model {key2} shape: {pre_dict[key1].shape}, {cur_dict[key2].shape}')\n",
    "#                 continue\n",
    "#             cur_dict[key2] = pre_dict[key1] \n",
    "#         msg = backbone.load_state_dict(cur_dict, strict=True)\n",
    "#         lines.append(f\"==> {msg}\")\n",
    "#     else:\n",
    "#         lines.append(f\"==> no weight found at '{cfg.resume_weight}'\")\n",
    "#     print('\\n'.join(lines))\n",
    "\n",
    "#     # freeze backbone\n",
    "#     for p in backbone.parameters():\n",
    "#         p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMN CWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.mmn import MMN as MMNOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Loading weight from path: /Users/nigel/Documents/Research-Git/Vis/pretrained/mmn-all/f11e_pm10/best.pth\n",
      "Loaded Weight Successfully\n"
     ]
    }
   ],
   "source": [
    "#  Trans\n",
    "Trans_old = MMNOLD(cfg, agg=cfg.agg, wa=cfg.wa, red_dim=cfg.red_dim).cpu()\n",
    "meta_model_weight_pth = os.path.join(META_MODEL_PATH, f\"{'t' if cfg.meta_aug>1 else 'f'}11e_pm{cfg.shot}{cfg.train_split}\", \"best.pth\")\n",
    "# meta_model_weight_pth = '/Users/nigel/Documents/Research-Git/Vis/pretrained/pascal_resnet50/mmn/best.pth' \n",
    "# load Trans weight\n",
    "pre_dict, cur_dict = torch.load(meta_model_weight_pth, map_location=torch.device('cpu'))['state_dict'], Trans_old.state_dict()\n",
    "print(f\"Start Loading weight from path: {meta_model_weight_pth}\")\n",
    "for key1, key2 in zip(list(pre_dict.keys())[1:], cur_dict.keys()):\n",
    "    if pre_dict[key1].shape != cur_dict[key2].shape:\n",
    "        print(f'Pre-trained {key1} shape and model {key2} shape: {pre_dict[key1].shape}, {cur_dict[key2].shape}')\n",
    "        continue\n",
    "    cur_dict[key2] = pre_dict[key1] \n",
    "# Trans.load_state_dict(trans_weight['state_dict'])\n",
    "classifier_key = list(pre_dict.keys())[0]\n",
    "backbone_dict[classifier_key] = pre_dict[classifier_key]\n",
    "print(f\"Loaded Weight Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMN PROTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmn.module.mmn import MMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Loading weight from path: /Users/nigel/Documents/Research-Git/Vis/pretrained/mmn-all/f11e_pm10/best.pth\n",
      "Pre-trained trans.corr_net.NeighConsensus.conv.0.conv1.weight shape and model corr_net.NeighConsensus.conv.0.conv1.weight shape: torch.Size([10, 2, 3, 3]), torch.Size([10, 3, 3, 3])\n",
      "Pre-trained trans.corr_net.NeighConsensus.conv.0.conv2.weight shape and model corr_net.NeighConsensus.conv.0.conv2.weight shape: torch.Size([10, 2, 3, 3]), torch.Size([10, 3, 3, 3])\n",
      "Loaded Weight Successfully\n"
     ]
    }
   ],
   "source": [
    "#  Trans\n",
    "Trans = MMN(cfg).cpu()\n",
    "meta_model_weight_pth = os.path.join(META_MODEL_PATH, f\"{'t' if cfg.meta_aug>1 else 'f'}11e_pm{cfg.shot}{cfg.train_split}\", \"best.pth\")\n",
    "# meta_model_weight_pth = '/Users/nigel/Documents/Research-Git/Vis/pretrained/pascal_resnet50/mmn/best.pth' \n",
    "# load Trans weight\n",
    "pre_dict, cur_dict = torch.load(meta_model_weight_pth, map_location=torch.device('cpu'))['state_dict'], Trans.state_dict()\n",
    "pre_key_list, cur_key_list = list(pre_dict.keys())[1:], list(cur_dict.keys())\n",
    "\n",
    "cur_key_list = filter(lambda x: not \"wa_2\" in x, cur_key_list)\n",
    "\n",
    "print(f\"Start Loading weight from path: {meta_model_weight_pth}\")\n",
    "for key1, key2 in zip(pre_key_list, cur_key_list):\n",
    "    if pre_dict[key1].shape != cur_dict[key2].shape:\n",
    "        print(f'Pre-trained {key1} shape and model {key2} shape: {pre_dict[key1].shape}, {cur_dict[key2].shape}')\n",
    "        continue\n",
    "    cur_dict[key2] = pre_dict[key1] \n",
    "# Trans.load_state_dict(trans_weight['state_dict'])\n",
    "classifier_key = list(pre_dict.keys())[0]\n",
    "backbone_dict[classifier_key] = pre_dict[classifier_key]\n",
    "print(f\"Loaded Weight Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['classifier.classifier.weight', 'trans.wa_3.conv_theta.weight', 'trans.wa_3.conv_theta.bias', 'trans.wa_3.conv_phi.weight', 'trans.wa_3.conv_phi.bias', 'trans.wa_3.conv_g.weight', 'trans.wa_3.conv_g.bias', 'trans.wa_3.conv_back.weight', 'trans.wa_3.conv_back.bias', 'trans.wa_4.conv_theta.weight', 'trans.wa_4.conv_theta.bias', 'trans.wa_4.conv_phi.weight', 'trans.wa_4.conv_phi.bias', 'trans.wa_4.conv_g.weight', 'trans.wa_4.conv_g.bias', 'trans.wa_4.conv_back.weight', 'trans.wa_4.conv_back.bias', 'trans.corr_net.NeighConsensus.conv.0.conv1.weight', 'trans.corr_net.NeighConsensus.conv.0.conv1.bias', 'trans.corr_net.NeighConsensus.conv.0.conv2.weight', 'trans.corr_net.NeighConsensus.conv.0.conv2.bias', 'trans.corr_net.NeighConsensus.conv.2.conv1.weight', 'trans.corr_net.NeighConsensus.conv.2.conv1.bias', 'trans.corr_net.NeighConsensus.conv.2.conv2.weight', 'trans.corr_net.NeighConsensus.conv.2.conv2.bias', 'trans.corr_net.NeighConsensus.conv.4.conv1.weight', 'trans.corr_net.NeighConsensus.conv.4.conv1.bias', 'trans.corr_net.NeighConsensus.conv.4.conv2.weight', 'trans.corr_net.NeighConsensus.conv.4.conv2.bias'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(pre_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['wa_2.conv_theta.weight', 'wa_2.conv_theta.bias', 'wa_2.conv_phi.weight', 'wa_2.conv_phi.bias', 'wa_2.conv_g.weight', 'wa_2.conv_g.bias', 'wa_2.conv_back.weight', 'wa_2.conv_back.bias', 'wa_3.conv_theta.weight', 'wa_3.conv_theta.bias', 'wa_3.conv_phi.weight', 'wa_3.conv_phi.bias', 'wa_3.conv_g.weight', 'wa_3.conv_g.bias', 'wa_3.conv_back.weight', 'wa_3.conv_back.bias', 'wa_4.conv_theta.weight', 'wa_4.conv_theta.bias', 'wa_4.conv_phi.weight', 'wa_4.conv_phi.bias', 'wa_4.conv_g.weight', 'wa_4.conv_g.bias', 'wa_4.conv_back.weight', 'wa_4.conv_back.bias', 'corr_net.NeighConsensus.conv.0.conv1.weight', 'corr_net.NeighConsensus.conv.0.conv1.bias', 'corr_net.NeighConsensus.conv.0.conv2.weight', 'corr_net.NeighConsensus.conv.0.conv2.bias', 'corr_net.NeighConsensus.conv.2.conv1.weight', 'corr_net.NeighConsensus.conv.2.conv1.bias', 'corr_net.NeighConsensus.conv.2.conv2.weight', 'corr_net.NeighConsensus.conv.2.conv2.bias', 'corr_net.NeighConsensus.conv.4.conv1.weight', 'corr_net.NeighConsensus.conv.4.conv1.bias', 'corr_net.NeighConsensus.conv.4.conv2.weight', 'corr_net.NeighConsensus.conv.4.conv2.bias'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(cur_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "from torch.optim.lr_scheduler import MultiStepLR, StepLR, CosineAnnealingLR\n",
    "\n",
    "\n",
    "def get_optimizer(\n",
    "        args: argparse.Namespace, parameters: List[nn.Module]\n",
    ") -> torch.optim.Optimizer:\n",
    "    if args.main_optim == 'SGD':\n",
    "        return torch.optim.SGD(\n",
    "            parameters, momentum=args.momentum,\n",
    "            weight_decay=args.weight_decay, nesterov=args.nesterov\n",
    "        )\n",
    "    elif args.main_optim == 'Adam':\n",
    "        return torch.optim.Adam(parameters, weight_decay=args.weight_decay)\n",
    "\n",
    "\n",
    "def get_scheduler(\n",
    "        args: argparse.Namespace,\n",
    "        optimizer: torch.optim.Optimizer, batches: int\n",
    ") -> torch.optim.lr_scheduler._LRScheduler:\n",
    "    \"\"\"\n",
    "    cosine will change learning rate every iteration, others change learning rate every epoch\n",
    "    :param batches: the number of iterations in each epochs\n",
    "    :return: scheduler\n",
    "    \"\"\"\n",
    "    SCHEDULERS = {\n",
    "        'step': StepLR(optimizer, args.lr_stepsize, args.gamma),\n",
    "        'multi_step': MultiStepLR(optimizer, milestones=args.milestones, gamma=args.gamma),\n",
    "        'cosine': CosineAnnealingLR(optimizer, batches * args.epochs, eta_min=1e-6),\n",
    "        None: None\n",
    "    }\n",
    "    return SCHEDULERS[args.scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as F_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary list for val loader\n",
    "\n",
    "with open(os.path.join(CLASS_LIST_PATH, \"data-list.json\"), 'r') as f:\n",
    "    cfg.data_list = json.load(f)\n",
    "\n",
    "with open(os.path.join(CLASS_LIST_PATH, \"sub-class.json\"), 'r') as f:\n",
    "    cfg.sub_class_file_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5953/5953 [00:01<00:00, 3367.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations applied: [<__main__.Resize object at 0x7fb7978e1350>, <__main__.ToTensor object at 0x7fb7978e1690>, <__main__.Normalize object at 0x7fb7978e1850>]\n",
      "INFO: pascal -> pascal\n",
      "INFO: 0 -> 0\n",
      ">> Start Filtering classes \n",
      ">> Removed classes = [] \n",
      ">> Kept classes = ['airplane', 'bicycle', 'bird', 'boat', 'bottle'] \n",
      "Processing data for [1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449/1449 [00:00<00:00, 6310.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations applied: [<__main__.Resize object at 0x7fb79747c1d0>, <__main__.ToTensor object at 0x7fb797a68050>, <__main__.Normalize object at 0x7fb7975c6b90>]\n"
     ]
    }
   ],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, cfg, train_loader=False, val_loader=True):\n",
    "        assert train_loader or val_loader, \"At lease one of train/val loader should be True\"\n",
    "        if (train_loader):\n",
    "            train_loader, _ = get_train_loader(cfg)\n",
    "            self.train = True\n",
    "        else:\n",
    "            print(f\"WARNING: Train loader disabled\")\n",
    "            train_loader = []\n",
    "            self.train = False\n",
    "        if (val_loader):\n",
    "            val_loader, _ = get_val_loader(cfg)\n",
    "            self.val = True\n",
    "        else:\n",
    "            print(f\"WARNING: Val loader disabled\")\n",
    "            val_loader = []\n",
    "            self.val = False\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        self.train_loader = iter(train_loader)\n",
    "        self.val_loader = iter(val_loader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.val_loader) if self.val_loader else len(self.train_loader)   \n",
    "\n",
    "    def next(self, train=False):\n",
    "        if (train):\n",
    "            assert self.train\n",
    "            qry_img, q_label, spt_imgs, s_label, subcls, spt_info, q_info = self.train_loader.next()\n",
    "        else:\n",
    "            assert self.val\n",
    "            qry_img, q_label, spt_imgs, s_label, subcls, spt_info, q_info = self.val_loader.next()\n",
    "\n",
    "        return qry_img, q_label, spt_imgs, s_label, subcls, spt_info, q_info \n",
    "\n",
    "    def reset(self):\n",
    "        print(f\"Start resetting loaders.\")\n",
    "        if (self.train):\n",
    "            train_loader, _ = get_train_loader(cfg)\n",
    "            self.train_loader = iter(train_loader)\n",
    "        if (self.val):\n",
    "            val_loader, _ = get_val_loader(cfg)\n",
    "            self.val_loader = iter(val_loader)\n",
    "        print(f\"Reset complete. Train: {self.train}; Val: {self.val}\")\n",
    "\n",
    "dataset_loader = DatasetLoader(cfg, train_loader=True, val_loader=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SegLoss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper:\n",
    "    def __init__(self, backbone: PSPNet, meta_model: MMN, cfg: CfgNode, dataset_loader: DatasetLoader, search_mode:int = 0):\n",
    "        # assert type(backbone)==PSPNet and type(meta_model)==MMN, \"Backbone should be instance of PSPNet, meta_model should be instance of MMN\"\n",
    "        self.backbone, self.meta_model, self.cfg, self.dataset_loader, self.search_mode = backbone, meta_model, cfg, dataset_loader, search_mode\n",
    "        # self.optimizer_meta = get_optimizer(self.cfg, [dict(params=self.meta_model.parameters(), lr=self.cfg.trans_lr * self.cfg.scale_lr)])\n",
    "        # self.scheduler = get_scheduler(self.cfg, self.optimizer_meta, len(self.dataset_loader))\n",
    "\n",
    "    def get_pred(self, data=None, omit_out=False):\n",
    "        data = data if data else self.dataset_loader.next(train=False)\n",
    "        assert len(data)==7, \"Data should be in the form of (qry_img, q_label, spt_imgs, s_label, subcls, spt_info, q_info)\"\n",
    "\n",
    "        # Shape:\n",
    "        #   qry_img: [1, 3, h, w]\n",
    "        #   q_label: [1, h, w]\n",
    "        #   spt_imgs: [1, shot & aug, 3, h, w]\n",
    "        #   s_label: [1, shot & aug, h, w]\n",
    "\n",
    "        qry_img, q_label, spt_imgs, s_label, subcls, spt_info, q_info = data\n",
    "\n",
    "        num_spt = spt_imgs.shape[1]\n",
    "\n",
    "        train_loss_meter0 = AverageMeter()\n",
    "        train_iou_meter0 = AverageMeter()\n",
    "\n",
    "        train_loss_meter1 = AverageMeter()\n",
    "        train_iou_meter1 = AverageMeter()\n",
    "\n",
    "        train_iou_compare = CompareMeter()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            spt_imgs = spt_imgs.cuda()\n",
    "            s_label = s_label.cuda()\n",
    "            q_label = q_label.cuda()\n",
    "            qry_img = qry_img.cuda()\n",
    "\n",
    "        # ====== Phase 1: Train a new binary classifier on support samples. ======\n",
    "        spt_imgs = spt_imgs.squeeze(0)\n",
    "        s_label = s_label.squeeze(0).long()\n",
    "\n",
    "        self.backbone.eval()\n",
    "        with torch.no_grad():\n",
    "            f_s, fs_lst = self.backbone.extract_features(spt_imgs)\n",
    "            \n",
    "        self.backbone.classifier.train()\n",
    "        if (self.search_mode==0):\n",
    "            self.backbone.inner_loop(f_s[0].unsqueeze(0), s_label[0].unsqueeze(0))\n",
    "        else:\n",
    "            self.backbone.inner_loop(f_s, s_label)\n",
    "\n",
    "        # ====== Phase 2: Update query score using attention. ======\n",
    "\n",
    "        self.backbone.eval()\n",
    "        criterion = SegLoss(loss_type=self.cfg.loss_type)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            f_q, fq_lst = self.backbone.extract_features(qry_img)\n",
    "            pred_q0 = self.backbone.classifier(f_q)\n",
    "            pred_q0 = F.interpolate(pred_q0, size=q_label.shape[1:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        if (self.search_mode==0):\n",
    "            self.backbone.classifier.train()\n",
    "            self.backbone.inner_loop(f_s, s_label)\n",
    "\n",
    "            self.backbone.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_q1 = self.backbone.classifier(f_q)\n",
    "                pred_q1 = F.interpolate(pred_q0, size=q_label.shape[1:], mode='bilinear', align_corners=True)\n",
    "\n",
    "            pred_q2 = pred_q1\n",
    "\n",
    "        else:\n",
    "            # adaptive attention for feature selection\n",
    "            if self.cfg.get('att_type', 2) == 0 or self.cfg.get('att_type', 2) == 1:\n",
    "                fs_lst = { k: [tensor_slice(e, idx=self.cfg.att_type) for e in v] for k, v in fs_lst.items() }\n",
    "                f_s = tensor_slice(f_s, idx=self.cfg.att_type)\n",
    "            elif self.cfg.get('att_type', 2) == 3:\n",
    "                with torch.no_grad():\n",
    "                    pred_s0 = self.backbone.classifier(f_s)\n",
    "                    pred_s0 = F.interpolate(pred_s0, size=s_label.shape[1:], mode='bilinear', align_corners=True)   # [B, 2, 473, 473]\n",
    "                    intersection, union, _ = batch_intersectionAndUnionGPU(pred_s0.unsqueeze(0), s_label.unsqueeze(0), num_classes=2, ignore_index=255)\n",
    "                    iou = torch.mean( (intersection / (union + 1e-10)).squeeze(0), dim=-1 )\n",
    "                    fs_lst = { k: [tensor_slice(e, ref=iou) for e in v] for k, v in fs_lst.items() }\n",
    "                    f_s = tensor_slice(f_s, ref=iou)\n",
    "            \n",
    "            # meta model forward & train\n",
    "            self.meta_model.train()\n",
    "            \n",
    "            att_fq = []\n",
    "            loss_sum = 0\n",
    "\n",
    "            for k in range(len(f_s)):\n",
    "                single_fs_lst = { key: [ve[k:k + 1] for ve in value] for key, value in fs_lst.items() }\n",
    "                single_f_s = f_s[k:k+1]\n",
    "                _, att_fq_single = self.meta_model(fq_lst, single_fs_lst, single_f_s)\n",
    "                att_fq.append(att_fq_single)\n",
    "\n",
    "            att_fq = torch.cat(att_fq, dim=0)\n",
    "            att_fq = att_fq.mean(dim=0, keepdim=True)\n",
    "            fq = f_q * (1-self.cfg.att_wt) + att_fq * self.cfg.att_wt\n",
    "\n",
    "            pred_q1 = self.backbone.classifier(att_fq)\n",
    "            pred_q1 = F.interpolate(pred_q1, size=q_label.shape[-2:], mode='bilinear', align_corners=True)\n",
    "            pred_q2 = self.backbone.classifier(fq)\n",
    "            pred_q2 = F.interpolate(pred_q2, size=q_label.shape[-2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        # Loss function: Dynamic class weights used for query image only during training\n",
    "        q_loss0 = criterion(pred_q0, q_label.long())\n",
    "        q_loss2 = criterion(pred_q2, q_label.long())\n",
    "\n",
    "        if self.cfg.loss_shot == 'avg':\n",
    "            q_loss1 = criterion(pred_q1, q_label.long())\n",
    "        else:   # 'sum'\n",
    "            q_loss1 = loss_sum\n",
    "        \n",
    "        if (self.cfg.get('aux', False)) == False:\n",
    "            loss = q_loss1\n",
    "        else:\n",
    "            loss = q_loss1 + self.cfg.aux * q_loss2\n",
    "\n",
    "        # self.optimizer_meta.zero_grad()\n",
    "        # loss.backward()\n",
    "        # self.optimizer_meta.step()\n",
    "        # if self.cfg.scheduler == 'cosine':\n",
    "        #     self.scheduler.step()\n",
    "\n",
    "        IoUb, IoUf = dict(), dict()                                               # IoU background, IoU foreground\n",
    "        for (pred, idx) in [(pred_q0, 0), (pred_q1, 1), (pred_q2, 2)]:\n",
    "            intersection, union, _ = intersectionAndUnionGPU(pred.argmax(1), q_label, self.cfg.num_classes_tr, 255)\n",
    "            IoUb[idx], IoUf[idx] = (intersection / (union + 1e-10)).cpu().numpy()  # mean of BG and FG\n",
    "\n",
    "        train_loss_meter0.update(q_loss0.item() / self.cfg.batch_size, 1)\n",
    "        train_iou_meter0.update((IoUf[0]+IoUb[0])/2, 1)\n",
    "        train_loss_meter1.update(q_loss1.item() / self.cfg.batch_size, 1)\n",
    "        train_iou_meter1.update((IoUf[1] + IoUb[1]) / 2, 1)\n",
    "        train_iou_compare.update(IoUf[1], IoUf[0])\n",
    "        \n",
    "        msg = f\"\"\"IoUf0={'{:2f}'.format(IoUf[0])}-----IoUb0={'{:2f}'.format(IoUb[0])}\n",
    "                IoUf1={'{:2f}'.format(IoUf[1])}-----IoUb1={'{:2f}'.format(IoUb[1])}\n",
    "                IoUf2={'{:2f}'.format(IoUf[2])}-----IoUb2={'{:2f}'.format(IoUb[2])}\n",
    "                loss0={'{:2f}'.format(q_loss0)}-----loss1={'{:2f}'.format(q_loss1)}\n",
    "                difference-(loss2-loss0)={'{:2f}'.format(q_loss2-q_loss0)}\n",
    "                support-path={spt_info[0][0][0]}\n",
    "                query-path={q_info[0][0]}\"\"\".replace('\\t', '').replace(' ', '')\n",
    "                # lr {'{:2f}'.format(self.optimizer_meta.param_groups[0]['lr'])}\n",
    "        if not (omit_out):\n",
    "            print(msg)\n",
    "\n",
    "        return pred_q0, pred_q1, pred_q2, spt_imgs, s_label, qry_img, q_label, spt_info, q_info, (q_loss0, q_loss1, q_loss2), (IoUf, IoUb)\n",
    "\n",
    "model = ModelWrapper(backbone, Trans, cfg, dataset_loader, SEARCH_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "\n",
    "    '''\n",
    "        Visualizer for displaying and saving visualizations\n",
    "\n",
    "        Args:\n",
    "\n",
    "            cfg (CfgNode): a JS Object like configuration instance for storing relevant configs\n",
    "\n",
    "            datasetloader (DatasetLoader): dataset loader instance for iteratively accessing episodic data\n",
    "\n",
    "            model (VisModelWrapper): a model instance that wraps backbone & meta model, providing useful accessing methods\n",
    "\n",
    "            transparency (float): set transparency for original images, default to 1.0\n",
    "\n",
    "\n",
    "        Attributes:\n",
    "\n",
    "            cfg (CfgNode): a JS Object like configuration instance for storing relevant configs\n",
    "\n",
    "            datasetloader (DatasetLoader): dataset loader instance for iteratively accessing episodic data\n",
    "\n",
    "            model (VisModelWrapper): a model instance that wraps backbone & meta model, providing useful accessing methods\n",
    "\n",
    "            transparency (float): set transparency for original images, default to 1.0\n",
    "\n",
    "\n",
    "        Methods:\n",
    "\n",
    "            shownext(self, train=False)\n",
    "                plot the next batch of episodic data, default using data from validation loader\n",
    "            \n",
    "            showprediction(self, train=False)\n",
    "                plot the prediction of next episodic data, default using data from validation loader\n",
    "\n",
    "            search(self, threshold: float, num_episodes: int, save_path: str, color_mode=\"R\")\n",
    "                search for suitable images\n",
    "            \n",
    "    '''\n",
    "\n",
    "    def __init__(self, cfg: CfgNode, datasetloader: DatasetLoader, model: ModelWrapper, transparency: float):\n",
    "        assert cfg is not None and type(cfg)==CfgNode, \"cfg should not be None and should be an instance of class CfgNode\"\n",
    "        assert datasetloader is not None and type(datasetloader)==DatasetLoader, \"datasetloader should not be None and should be an instance of class DatasetLoader\"\n",
    "        assert model is not None and type(model)==ModelWrapper, \"model should not be None and should be an instance of class ModelWrapper\"\n",
    "        assert transparency is not None and type(transparency) in (float, int), \"transparency should not be None and should be either integer OR float\"\n",
    "\n",
    "        self.cfg, self.datasetloader, self.model, self.transparency = cfg, datasetloader, model, transparency\n",
    "\n",
    "\n",
    "    def _plot(self, img: torch.Tensor) -> None:\n",
    "\n",
    "        '''\n",
    "            Utility method for plotting a single image\n",
    "\n",
    "            Args:\n",
    "\n",
    "                img (torch.Tensor): an image of type torch.Tensor, shape should be (3, h, w) OR (h, w, 3)\n",
    "\n",
    "            Returns: \n",
    "\n",
    "                None\n",
    "        '''\n",
    "\n",
    "        img = img.squeeze()\n",
    "\n",
    "        if img.shape[0] == 3:\n",
    "            plt.imshow(img.permute(1, 2, 0))\n",
    "        else:\n",
    "            plt.imshow(img)\n",
    "\n",
    "    def _plot_list(self, img_list: list, spt_path: str, qry_path: str, save_path=None) -> None:\n",
    "\n",
    "        '''\n",
    "            Utility method for plotting a list of images\n",
    "\n",
    "            Args:\n",
    "\n",
    "                img_list (list): a list of dict, each instance contraining a dict like { 'img': <img_tensor>, 'label': <str_label> }\n",
    "\n",
    "            Returns: \n",
    "            \n",
    "                None\n",
    "        '''\n",
    "\n",
    "        print(f\"Support Path: {spt_path}\\nQuery Path: {qry_path}\\n----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        f = plt.figure(figsize=(20, 20*len(img_list)), dpi=300)\n",
    "\n",
    "        for i in range(len(img_list)):\n",
    "            print(img_list[i]['label'])\n",
    "            plt.subplot(1, len(img_list), i+1)\n",
    "            plt.axis('off')\n",
    "            self._plot(img_list[i]['img'])\n",
    "\n",
    "        if (save_path):\n",
    "            f.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "\n",
    "    def _get_masked_image(self, img: torch.Tensor, mask: torch.Tensor, mode=\"B\") -> torch.Tensor:\n",
    "\n",
    "        '''\n",
    "            Utility method for getting an image with specified mask\n",
    "\n",
    "            Args:\n",
    "\n",
    "                img (torch.Tensor): an image of type torch.Tensor, shape should be (h, w, 3)\n",
    "\n",
    "                mask (torch.Tensor): a mask of type torch.Tensor, shape should be (h, w)\n",
    "\n",
    "                mode (str): 'R','G','B' red, green, blue\n",
    "\n",
    "            Returns:\n",
    "\n",
    "                masked_img (torch.Tensor): a new image with mask applied\n",
    "\n",
    "            Raises:\n",
    "\n",
    "                RuntimeWarning: Mode should be 'R','G','B'; defaulting to use mode 'R'\n",
    "        '''\n",
    "\n",
    "        assert len(img.shape) == 3 and (len(mask.shape) == 2 or len(mask.shape) == 3) , \"Img/Mask shape invalid\"\n",
    "        mask = mask.squeeze()\n",
    "        mask = rearrange(mask, 'h w -> (h w)')\n",
    "        mask[torch.where(mask==255)[0]] = 0\n",
    "        mask = rearrange(mask, '(h w) -> h w', h=473).unsqueeze(2)\n",
    "\n",
    "        n = torch.zeros(mask.shape)\n",
    "\n",
    "        if mode == 'R':\n",
    "            mask = torch.cat((mask, n, n), dim = 2)\n",
    "        elif mode == 'G':\n",
    "            mask = torch.cat((n, mask, n), dim = 2)\n",
    "        elif mode == 'B':\n",
    "            mask = torch.cat((n, n, mask), dim = 2)\n",
    "        else:\n",
    "            mask = torch.cat((mask, n, n), dim = 2)\n",
    "            raise RuntimeWarning(\"Mode should be 'R','G','B'; defaulting to use mode 'R'\")\n",
    "        \n",
    "        masked_img = mask + img * self.transparency\n",
    "        \n",
    "        return masked_img\n",
    "\n",
    "    def shownext(self, train=False, save_path=None):\n",
    "        '''\n",
    "            Displaying next episode images\n",
    "\n",
    "            Args:\n",
    "\n",
    "                train (bool): whether to use data from train loader\n",
    "        '''\n",
    "\n",
    "        # Shape:\n",
    "        #   qry_img: [1, 3, h, w]\n",
    "        #   q_label: [1, h, w]\n",
    "        #   spt_imgs: [1, shot & aug, 3, h, w]\n",
    "        #   s_label: [1, shot & aug, h, w]\n",
    "\n",
    "\n",
    "        qry_img, q_label, spt_imgs, s_label, subcls, spt_info, q_info = self.datasetloader.next(train)\n",
    "\n",
    "        s_label = s_label.squeeze(0)\n",
    "        q_label = q_label.squeeze(0)\n",
    "\n",
    "        spt_imgs = spt_info[2].squeeze()\n",
    "        spt_imgs = rearrange(spt_imgs, 'c h w -> h w c')\n",
    "        qry_img = q_info[2].squeeze()\n",
    "        qry_img = rearrange(qry_img, 'c h w -> h w c')\n",
    "\n",
    "        qry_path = q_info[0][0]\n",
    "        spt_path = spt_info[0][0][0]\n",
    "\n",
    "        masked_spt_im = self._get_masked_image(spt_imgs, s_label)\n",
    "\n",
    "        img_list = [\n",
    "            {'img': qry_img, 'label': f\"Query Image\"},\n",
    "            {'img': spt_imgs, 'label': f\"Original Support Image\"},\n",
    "            {'img': masked_spt_im, 'label': f\"Support Image with Mask\"},\n",
    "        ]\n",
    "\n",
    "        self._plot_list(img_list, spt_path, qry_path)\n",
    "\n",
    "    def shownextpred(self, save_path=None):\n",
    "        pred_q0, pred_q1, pred_q2, spt_imgs, s_label, qry_img, q_label, spt_info, q_info, loss_info = self.model.get_pred(data=self.datasetloader.next())\n",
    "\n",
    "        spt_imgs = spt_info[2][0].squeeze().detach()\n",
    "        spt_imgs = rearrange(spt_imgs, 'c h w -> h w c')\n",
    "        qry_img = q_info[2].squeeze().detach()\n",
    "        qry_img = rearrange(qry_img, 'c h w -> h w c')\n",
    "\n",
    "        qry_path = q_info[0][0]\n",
    "        spt_path = spt_info[0][0][0]\n",
    "\n",
    "        masked_spt_im = self._get_masked_image(spt_imgs, s_label[0].squeeze().detach())\n",
    "\n",
    "        pred_q0 = torch.argmax(pred_q0.detach(), dim=1).squeeze(0)\n",
    "        pred_q1 = torch.argmax(pred_q1.detach(), dim=1).squeeze(0)\n",
    "        pred_q2 = torch.argmax(pred_q2.detach(), dim=1).squeeze(0)\n",
    "\n",
    "        masked_qry_img_truth = self._get_masked_image(qry_img, q_label)\n",
    "        masked_qry_img0 = self._get_masked_image(qry_img, pred_q0)\n",
    "        masked_qry_img2 = self._get_masked_image(qry_img, pred_q2)\n",
    "\n",
    "        img_list = [\n",
    "            {'img': spt_imgs, 'label': f\"Original Support Image\"},\n",
    "            {'img': masked_spt_im, 'label': f\"Support Image with Mask\"},\n",
    "            {'img': qry_img, 'label': f\"Query Image\"},\n",
    "            {'img': masked_qry_img_truth, 'label': f\"Query Image with Mask Ground Truth\"},\n",
    "            {'img': masked_qry_img0, 'label': f\"Query Image with Mask 0\"},\n",
    "            {'img': masked_qry_img2, 'label': f\"Query Image with Mask 2\"},\n",
    "        ]\n",
    "\n",
    "        self._plot_list(img_list, spt_path, qry_path, save_path)\n",
    "\n",
    "\n",
    "    def search(self, threshold: float, num_episodes: int, save_path: str, color_mode:str=\"R\", search_mode:int=0) -> None:\n",
    "\n",
    "        '''\n",
    "            Search from validation data and retain episodes that satisfy required decision threshold\n",
    "\n",
    "            Args:\n",
    "\n",
    "                threshold (float): decision threshold -- episodes that satisfies `MAX(pred1-pred0, pred2-pred0) > threshold` will be retained\n",
    "\n",
    "                num_episodes (int): number of episodic data to test before stop\n",
    "\n",
    "                save_path (str): directory path to save result images\n",
    "\n",
    "                color_mode (str): mask color\n",
    "\n",
    "                search_mode (int): determines which search mode to use\n",
    "\n",
    "            Raises:\n",
    "\n",
    "                AssertionError: type does not match/tensor shape does not match\n",
    "\n",
    "                RuntimeError: Any\n",
    "\n",
    "            Returns:\n",
    "\n",
    "                None\n",
    "        '''\n",
    "        assert type(num_episodes)==int, 'Number of episodes should be of type int'\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            omit_out = not bool(save_path)\n",
    "            pred_q0, pred_q1, pred_q2, spt_imgs, s_label, qry_img, q_label, spt_info, q_info, loss_info, IoU_info = self.model.get_pred(data=self.datasetloader.next(), omit_out=omit_out)\n",
    "\n",
    "            if (IoU_info[0][2]+IoU_info[1][2] - (IoU_info[0][0]+IoU_info[1][0])>=2*threshold):\n",
    "                original_spt_imgs = spt_info[2][0].squeeze().detach()\n",
    "                original_spt_imgs = rearrange(original_spt_imgs, 'c h w -> h w c')\n",
    "                qry_img = q_info[2].squeeze().detach()\n",
    "                qry_img = rearrange(qry_img, 'c h w -> h w c')\n",
    "\n",
    "                qry_path = q_info[0][0]\n",
    "                spt_path = spt_info[0][0][0]\n",
    "\n",
    "                masked_spt_im = self._get_masked_image(original_spt_imgs, s_label[0].squeeze().detach(), mode=color_mode)\n",
    "\n",
    "                if (search_mode!=1):\n",
    "                    augmented_spt_img = spt_imgs.squeeze(0)[1]\n",
    "                    augmented_spt_img = rearrange(augmented_spt_img, 'c h w -> h w c')\n",
    "                    masked_augmented_spt_im = self._get_masked_image(augmented_spt_img, s_label[0].squeeze().detach(), mode=color_mode)\n",
    "                \n",
    "\n",
    "                pred_q0 = torch.argmax(pred_q0.detach(), dim=1).squeeze(0)\n",
    "                pred_q1 = torch.argmax(pred_q1.detach(), dim=1).squeeze(0)\n",
    "                pred_q2 = torch.argmax(pred_q2.detach(), dim=1).squeeze(0)\n",
    "\n",
    "                masked_qry_img_truth = self._get_masked_image(qry_img, q_label, mode=color_mode)\n",
    "                masked_qry_img0 = self._get_masked_image(qry_img, pred_q0, mode=color_mode)\n",
    "                masked_qry_img2 = self._get_masked_image(qry_img, pred_q2, mode=color_mode)\n",
    "\n",
    "                if (search_mode!=1):\n",
    "                    img_list = [\n",
    "                        {'img': original_spt_imgs, 'label': f\"Original Support Image\"},\n",
    "                        {'img': masked_spt_im, 'label': f\"Support Image with Mask\"},\n",
    "                        {'img': masked_augmented_spt_im, 'label': f\"Augmented Support Image with Mask\"},\n",
    "                        {'img': qry_img, 'label': f\"Query Image\"},\n",
    "                        {'img': masked_qry_img_truth, 'label': f\"Query Image with Mask Ground Truth\"},\n",
    "                        {'img': masked_qry_img0, 'label': f\"Query Image with Mask 0\"},\n",
    "                        {'img': masked_qry_img2, 'label': f\"Query Image with Mask 2\"},\n",
    "                    ]\n",
    "                else:\n",
    "                    img_list = [\n",
    "                        {'img': original_spt_imgs, 'label': f\"Original Support Image\"},\n",
    "                        {'img': masked_spt_im, 'label': f\"Support Image with Mask\"},\n",
    "                        {'img': qry_img, 'label': f\"Query Image\"},\n",
    "                        {'img': masked_qry_img_truth, 'label': f\"Query Image with Mask Ground Truth\"},\n",
    "                        {'img': masked_qry_img0, 'label': f\"Query Image with Mask 0\"},\n",
    "                        {'img': masked_qry_img2, 'label': f\"Query Image with Mask 2\"},\n",
    "                    ]\n",
    "\n",
    "                self._plot_list(img_list, spt_path, qry_path, f'{save_path}/{i}.png')\n",
    "\n",
    "\n",
    "visualizer = Visualizer(cfg, dataset_loader, model, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEARCH_MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b1/ydxkl81x4h50_7vtvlnmljb00000gn/T/ipykernel_49035/1711588756.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/Users/nigel/Documents/Research-Git/Vis/result_imgs/{datetime.now().strftime('%Y-%m-%d--%H-%M-%S')}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvisualizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOLOR_MODE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEARCH_MODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/b1/ydxkl81x4h50_7vtvlnmljb00000gn/T/ipykernel_49035/174205641.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, threshold, num_episodes, save_path, color_mode, search_mode)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0momit_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mpred_q0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_q2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspt_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqry_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspt_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIoU_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasetloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momit_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0momit_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIoU_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mIoU_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIoU_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mIoU_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/b1/ydxkl81x4h50_7vtvlnmljb00000gn/T/ipykernel_49035/158303239.py\u001b[0m in \u001b[0;36mget_pred\u001b[0;34m(self, data, omit_out)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0msingle_fs_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mve\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mve\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfs_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0msingle_f_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_fq_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfq_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_fs_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_f_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0matt_fq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt_fq_single\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/computer_vision/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research-Git/Vis/mmn/module/mmn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mf_q, mf_s, f_s)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mfs_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rd_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs_fea\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mfq_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wa_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfq_fea\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mfs_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wa_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs_fea\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_corr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfq_fea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_fea\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# [B, N_q, N_s]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/computer_vision/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research-Git/Vis/mmn/module/mmn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mpadded_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replicate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         neighbor = F.unfold(padded_x, kernel_size=self.R,\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "save_path = f\"/Users/nigel/Documents/Research-Git/Vis/result_imgs/{datetime.now().strftime('%Y-%m-%d--%H-%M-%S')}\"\n",
    "os.mkdir(save_path)\n",
    "visualizer.search(0.05, 2, save_path, COLOR_MODE, SEARCH_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('computer_vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fdc2f73bf5d480b4cf34a598d9b45a8492d840372cfa5913d0a708f31ebf008"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
